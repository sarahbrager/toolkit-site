[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Resources if they want to learn more.",
    "crumbs": [
      "Additional Resources"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Intro to Data Journalism",
    "section": "",
    "text": "What is data journalism?\nData journalism, or data-driven storytelling, is a form of journalism where reporters/other members of a news organization sift through large data sets to uncover trends or other newsworthy information, and then present those findings to the public in a digestible way. Simply put, it’s telling the stories found in data. In reality, though, data journalism is much more creative and complex. It can take a variety of forms, including:\n\nDesigning visualizations and interactive news graphics to explain numbers (These can often be embedded within news articles.)\nUsing code to quickly sort through large data sets and pull out the most important information\nReporting investigative stories based on findings from a data analysis (You might find something important that’s been completely overlooked!)\nMachine learning and AI engineering for newsrooms (This method is more computer science-oriented.)\n\nExamples of data journalism:",
    "crumbs": [
      "Data Journalism Basics"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Project",
    "section": "",
    "text": "This tutorial is for journalists, or other folks, who have large data sets on their hands and want to analyze them\nFor simplicity, the data used in this toolkit does not include bills filed in Special Session.\n\nA credit to the data source\nAll of the data for this project came from LegiScan [link], a real-time and nonpartisan legislative tracking service available to the public. While this tutorial only focuses on data from the Texas Legislature, LegiScan offers data sets for all 50 states. The data is updated weekly on Sundays.",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "reporting",
    "section": "",
    "text": "This will have markdown text with story suggestions and whatnot.",
    "crumbs": [
      "Reporting With This Data"
    ]
  },
  {
    "objectID": "sourcing.html",
    "href": "sourcing.html",
    "title": "sourcing",
    "section": "",
    "text": "Markdown instructions for sourcing/downloading the data we’re working with.",
    "crumbs": [
      "Sourcing Our Data"
    ]
  },
  {
    "objectID": "charting.html",
    "href": "charting.html",
    "title": "Charting The Data",
    "section": "",
    "text": "This is a markdown with instructions for using Datawrapper to make three simple charts from the data analysis.",
    "crumbs": [
      "Making Simple News Graphics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reporting on the Texas Legislature: A Data Journalism Toolkit",
    "section": "",
    "text": "An introduction to data journalism for political reporting including tips for sourcing raw data, using simple code for data analysis, and generating innovative storytelling ideas from the analysis.\nNote: This is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analyzing The Data",
    "section": "",
    "text": "Before starting a data analysis, it’s a good idea to brain dump a series of questions you hope to answer based on the variables you know you’re working with. For this toolkit, we’re going to answer the following:\n\nWhich Texas lawmakers filed the most bills? What does that look like broken down by political party?\n\nWe’ll also look at who has filed the least\n\nFrom which districts do the lawmakers who have filed the most come from?\nWhat does the number of bills filed look like in each month? Which month had the most?\nHow many bills can we flag for some of the biggest policy issues in Texas right now: abortion, immigration, education, energy/environment, and transgender issues?\n\nWe’ll flag bills that use these terms explicitly, as well as bills that use related terms\n\nHow do the stats above compare to the 88th legislative session?\n\nThis is not an exhaustive list of the questions you could answer with this data, but it’s a good introduction to sorting through large data sets. Let’s take a look at these questions one at a time.\n\n1. Setup again\nAs with your cleaning file, you need to import the libraries that allow you to run certain functions. We’ll be loading the same ones. Copy the code below and run it.\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\n\n\n\n2. Importing the clean data\nCopy and run the code below to import the table we saved in the previous step. To view the table again, type “final_table” on a new line in the code chunk and run it again. There should be 14,190 rows of data with nine columns: people_id, name, party, role, district, bill_id, bill_number, date_filed, and description. Right now, you can see the data is listed in alphabetical order by lawmaker last name, which is why Rep. Alma Allen comes up for the first 50 rows. In the next few steps, we’ll use different functions to rearrange the table and look at the data in new ways!\n\nfinal_table &lt;- read_rds(\"data-processed/bills-89.rds\")\n\nfinal_table\n\n\n\n3. Lawmakers: most vs. least filed\nNow let’s get into our first question: which lawmaker filed the most bills this session? To figure this out, we’re going to use three functions — group_by, summarize and arrange.\n\n“group_by” allows us to sort the data into specific groups based on variables that we want to focus on; in this case, we want to group by name, party, role, and district.\n“summarize” computes a summary of the data usually by a mean or a sum; we’ll just be focusing on the sum aspect because we want to count the number of bills each lawmaker has filed.\n“arrange” allows us to determine how we want to data to be presented; we can use this function to order the lawmaker names from most filed to least filed\n\nIt’s very common to use these three functions together to run basic computations of data. So let’s apply it to our data set! Run the code below to find out who filed the most bills.\n\n3.1 Doing the most first\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed))\n\nLooks like Sen. Bryan Hughes filed the most this session, with 276 bills. If you click through the table, you’ll see the values in the “number_filed” column decrease. That’s because we added the “desc()” element to our arrange function, which means we want it arranged in descending order.\n\n\n3.2 Just the top of the list\nWe’re looking at a lot of rows here, 180 to be exact, so what if we wanted to easily focus on the top 10 lawmakers who have filed the most this session? We can do this with the head() function. Run the code again, this time with one extra line at the end.\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed)) |&gt; \n  head(10)\n\nBecause we put the number 10 inside the head function, the table now only shows the first 10 rows – or top 10 lawmakers based on number of bills filed. Feel free to mess around with the number inside the head function and see how your result changes, such as finding the top 25 or top 50.\n\n\n3.3 Now the least\nWe can easily see who filed the least bills with one small change to the code above. The arrange() function defaults to ascending order, so least filed to most filed, when we don’t have that desc() specification in the code. So all we need to do is take that out and run it!\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed)\n\nRep. Ramon Romero Jr. filed the least this session, with only 10 bills. Like before, let’s just focus on the top 10 rows, which in this case would be the 10 lawmakers who filed the least bills this session. Copy and run the code below.\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed) |&gt; \n  head(10)\n\nCongrats! You just answered the first questions from the brainstorm. And hopefully you’re starting to realize how you can organize a data table and look at it in different ways.\n\n\n\n4. Districts\n\n\n5. Bills per month\n\n\n6. Policy topics\n\n\n7. This session vs. last session\n\n\n8. Recap of what we learned",
    "crumbs": [
      "Analyzing the Data"
    ]
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Cleaning The Data",
    "section": "",
    "text": "Some sort of introductory text to what cleaning data even means.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#setup",
    "href": "cleaning.html#setup",
    "title": "Cleaning The Data",
    "section": "1. Setup",
    "text": "1. Setup\nFirst, we need to load the packages that are going to help facilitate the data analysis. You don’t need to worry about what each package does, but this is an important setup step that allows us to use specific functions later on. I always include these three at the top of every document. Copy and paste the code below and “run” it by hitting the green play button in the top right of your code chunk.\n\n# setup \nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)\n\nYou’ll probably see some sort of message pop up after you run the code; as long as there’s no red “error” warning, you’re fine to move on.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#importing-the-data-from-legiscan",
    "href": "cleaning.html#importing-the-data-from-legiscan",
    "title": "Cleaning The Data",
    "section": "2. Importing the data from LegiScan",
    "text": "2. Importing the data from LegiScan\nNow you need to take the data that we downloaded to your computer and import it into this R file. It sounds complicated, but we can actually do this with a simple line of code.\nIf you open the files we downloaded from LegiScan in the previous step, you’ll see it actually includes seven different data tables. For the purposes of this project, we’re only going to be working with three of those: “bills,” “sponsors,” and “people.” Copy and run the code below to import those three tables into this file.\n\n bills &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/bills.csv\")\n \n bills\n\n\nsponsors &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/sponsors.csv\")\n\nsponsors\n\n\nlawmakers &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/people.csv\")\n\nlawmakers",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#section",
    "href": "cleaning.html#section",
    "title": "Cleaning The Data",
    "section": "3.",
    "text": "3.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#creating-a-pretty-table-to-work-with",
    "href": "cleaning.html#creating-a-pretty-table-to-work-with",
    "title": "Cleaning The Data",
    "section": "3. Creating a pretty table to work with",
    "text": "3. Creating a pretty table to work with",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#cleaning-those-tables",
    "href": "cleaning.html#cleaning-those-tables",
    "title": "Cleaning The Data",
    "section": "3. “Cleaning” those tables",
    "text": "3. “Cleaning” those tables\nOftentimes, data tables aren’t in the best shape when you download them. There might be rows or cells that are missing entries, or there may be columns that aren’t necessary for the analysis you want to do. This is where cleaning comes in: you can use code to adjust the table/remove data you don’t need so the final table is much easier to work with.\n\n3.1 Bills\nLet’s clean up each of the data tables one at a time. We’ll start with our “bills” data.\n\nA) View the raw data\nBefore we start cleaning up the tables, let’s take another look at what they look like untouched. Run this code to view the table you just imported.\n\nbills\n\nadd description of what we’re looking at here\n\n\nB) Remove unnecessary columns and rename columns\n\nbills_clean &lt;- bills |&gt; \n   select(-session_id, -status, -status_desc, -title, -committee_id, -committee, -last_action_date, -last_action, -url, -state_link) |&gt; \n  rename(date_filed = status_date)\n\nbills_clean \n\nDescribe what we’re looking at now.\n\n\n\n3.2 Lawmakers\n\nA) View the raw data\n\nlawmakers \n\nDescription of what we’ve got here.\n\n\nB) Remove unnecessary columns and rename columns\n\nlawmakers_clean &lt;- lawmakers |&gt; \n  select(people_id, name, party, role, district) \n\nlawmakers_clean\n\nDescription of what we’re working with now.\n\n\n\n3.3 Sponsors\n\nA) View the raw data\n\nsponsors\n\n\n\nB) Remove unnecessary columns and rename columns\n\nsponsors |&gt; \n  select(-position)\n\nRestate all the changes made.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#saving-the-clean-table",
    "href": "cleaning.html#saving-the-clean-table",
    "title": "Cleaning The Data",
    "section": "5. Saving the clean table",
    "text": "5. Saving the clean table\nAdd instructions for creating a data-processed file.\n\nfinal_table |&gt; \n  write_rds(\"data-processed/bills-89.rds\")\n\nIn the next chapter of this toolkit, we’ll brain storm some potential questions about the session and use the data to find some answers.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#merge-the-tables-together",
    "href": "cleaning.html#merge-the-tables-together",
    "title": "Cleaning The Data",
    "section": "4. Merge the tables together",
    "text": "4. Merge the tables together\nNow that we’ve selected the columns we want to work with in each of the three data tables, we need to merge everything together into one big table that we’ll use for our analysis. You’ll notice there are a couple of repeat columns in the tables, such as people_id and bill_id. We’ll want to make sure those only appear once in our big table. Let’s do this in two steps: first, copy the code below to merge the lawmaker and sponsor tables together.\n\nlawmaker_sponsor &lt;- merge(lawmakers_clean, sponsors) |&gt; \n  select(-position)\n\nlawmaker_sponsor\n\nNow lets add the description column and date_filed column from the bills table.\n\nfinal_table &lt;- lawmaker_sponsor |&gt; \n  left_join(bills_clean, by = join_by(bill_id)) \n\nfinal_table\n\nGreat! You should have one data table with the following information:\n\nname = lawmaker’s name\npeople_id = the unique number associated with each lawmaker\nparty = political party for each lawmaker\nrole = whether the lawmaker is a senator or a representative\ndistrict = the congressional district associated with each lawmaker\nbill_id = the unique number associated with a specific bill filed this session\ndate_filed = the date that bill was introduced\ndescription = explains what a specific bill aims to do\n\nWe’ll be able to quickly sort through all of this information and identify key details about the agendas of Texas lawmakers in the current legislative session. Let’s save this table before we move forward with our analysis.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "about.html#a-credit-to-the-data-source",
    "href": "about.html#a-credit-to-the-data-source",
    "title": "About This Project",
    "section": "A credit to the data source",
    "text": "A credit to the data source\nInclude some sort of disclaimer that all data for this project came from LegiScan; link to site",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "about.html#about-the-author",
    "href": "about.html#about-the-author",
    "title": "About This Project",
    "section": "About the author",
    "text": "About the author\nThis is an independent project by Sarah Brager, a senior journalism student at The University of Texas at Austin. She created this project as her senior capstone for the Moody College Honors Program and UT’s Bridging Disciplines Program, where she’s also earning a certificate in public policy studies. Sarah is a data journalism fellow with UT’s Media Innovation Group. To learn more about Sarah and see some of her other work, view her LinkedIn at the top right of this website.",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "Meet the Author",
    "section": "",
    "text": "Sarah Brager is a senior journalism student at The University of Texas at Austin. She created this project as her senior capstone for the Moody College Honors Program and Bridging Disciplines Program at UT, where she’s also earning a certificate in public policy studies. She is currently a data journalism fellow with UT’s Media Innovation Group. To learn more about Sarah and view her other work, visit her LinkedIn at the top right of this website.",
    "crumbs": [
      "Meet the Author"
    ]
  }
]