[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Resources if they want to learn more.",
    "crumbs": [
      "Additional Resources"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Intro to Data Journalism",
    "section": "",
    "text": "What is it?\nData journalism, or data-driven storytelling, is a form of journalism that involves sifting through large data sets to uncover trend and answer questions, and then presenting those findings to the public as you would any other type of newsworthy information. Simply put, it’s telling stories found in data.\nData-driven storytelling can be hard-hitting investigations or fun xyz. There’s no one right way of data reporting, and it’s always changing as digital tools become more advanced and accessible. Data journalism can take a variety of forms, including:\n\nDesigning informative and interactive visualizations to embed within news stories\nUsing code to quickly sort through large data sets and pull out the most important information\nReporting investigative stories based on findings from a data analysis (You might find something important that’s been completely overlooked!)\nMachine learning and AI engineering for newsrooms (A more computer science-oriented method)\n\n\n\nExamples of data reporting\nMany news publications have specific data departments that focus on investigation, analytics and visualization, including (but not limited to):\n\nProPublica\nThe New York Times\nThe Washington Post\nThe Texas Tribune\nThe Houston Chronicle\nThe Minnesota Star Tribune\n\n\n\nWhy is it useful?",
    "crumbs": [
      "What is Data Journalism?"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Getting Started with R",
    "section": "",
    "text": "Video tutorial on setting up workspace in posit.cloud.\n\nWhat is R?\nThis tutorial will introduce you to basic data analysis that can be done in R, which is one of many programming languages used by data journalists to better understand large, complex data sets. While this toolkit will show you some of R’s capabilities, it is by no means a complete look at how R can be used. If you like the activities in this toolkit, I highly recommend you check out the additional resources provided on this website.\n\nDo I need coding experience to use this toolkit?\nNope! An interest in stats is helpful, but this tutorial is designed for people who are new to data analysis and want an example of how news reporters can use data in their work. It may take a minute to adjust to the interface and style of R, but this lesson keeps things simple and user-friendly.\n\n\n\nFirst steps\n\n1. Go to posit.cloud and create an account\nThis is an online version of R, meaning you’ll be able to work with the same interface as the R desktop application but in your web browser. If you already R Studio installed on your computer or would prefer to use the desktop version, that will also work for this toolkit. The instructions will work either way. If you plan on pursuing future data projects beyond this tutorial, I do recommend installing R Studio because you will eventually hit a paywall in posit.cloud.\n\nInstalling R Studio (optional)\ntktk instructions for that\nIf you want to keep things simple, though, posit.cloud will do just fine! If you decide you want to install R Studio in the future, you can always come back to these instructions.\n\n\n\n2. Create a new project within posit.cloud\nOnce you’re logged in to your account, you should see a dashboard that lists all of your content (this is empty at the moment, of course). In the top right corner, click on the drop down button the create a new project and select “New RStudio Project.” All of the work you’ll do for this toolkit will be within this project, so if you leave and come back to these instructions make sure to re-enter the same project through your dashboard.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re working in the desktop version of RStudio, create a new project by selecting File —&gt; New Project —&gt;\n\n\n\n\n3. Name your project/familiarize yourself with the interface\nNow that you’re inside the project, immediately give it a name at the top of the workspace. I recommend something short and related to the data we’ll be working with, like “tx-lege-toolkit”\n\nNow you can see what the RStudio workspace looks like! It can be a a lot to take in at first, but you’ll adjust to the layout once you start creating files and working in them. Click on the boxes shown in the photo above the collapse the console, creating a fourth window where your files will eventually appear.\nThese are the 4 main windows within any R project: file workspace (top left), console/terminal (bottom left), environment (top right), and file list (bottom right). You can adjust the size of each window by hovering your cursor between them.\n\n\n\n4. Adding to your library\n\nFirst, folders\nBefore we download the data, let’s create two folders where you’ll drop the files. This is a good practice for keeping your files organized in any R project you work in. In your files window (bottom right), click the icon in the top left corner to create and name a new folder. Title the first one “data-raw” and then create a second folder called “data-processed”\n\nOnce you’ve created those, you’ll see them appear in your files list. The data-raw folder is where you’ll eventually put the unedited dataset that we’re going to download in the next step. Any data that you clean up/edit should be stored in the data-processed folder. It’s always good to keep a copy of the original data separate from the version that we’re going to edit and analyze, hence the two folders. Keep this in mind for any future data projects you decide to pursue!\n\n\nNow, the Quarto files\nWe’re also going to create two files inside which we’ll run code to analyze the data. Click the new file icon and select “Quarto Document.” Name one “cleaning.qmd” and then create another and call it “analysis.qmd.” Both of those should appear in your files list. We’ll come back to these later on in the tutorial.\n\n\n\n\n\nRecap\nIn this section, you learned how to create a new project in posit.cloud and R Studio and add files within that project. You should have two folders, data-raw and data-processed, and two files, one for cleaning and one for analysis.\nNow that you’ve set up your project, you’re ready to start working with the data! Feel free to return to these setup instructions for any other project you decide to create in posit.cloud or the R Studio app.\nIn the next section, we’re going to download the dataset of bills from the Texas Legislative session and add that data to your R project.",
    "crumbs": [
      "Setting Up"
    ]
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "reporting",
    "section": "",
    "text": "This will have markdown text with story suggestions and whatnot.",
    "crumbs": [
      "Reporting With This Data"
    ]
  },
  {
    "objectID": "sourcing.html",
    "href": "sourcing.html",
    "title": "Downloading Data from LegiScan",
    "section": "",
    "text": "Include a video tutorial at the top of this page\nAll of the data for this tutorial comes from LegiScan, a real-time and nonpartisan legislative tracking service available to the public. While this project only focuses on data from the Texas Legislature, LegiScan offers data sets from all 50 states. The data is updated weekly on Sundays.\nLegiScan is a good example of the type of public database that a data journalist might pull from frequently. There are lots of other resources available online with data from government agencies. If you’re a journalist covering the environment, for example, you might try xyz.\n\nSteps for downloading\n\n1. Go to legiscan.com\n\n\n2. Create an account\nLegiScan requires you to create a username and password to download datasets. This is completely free. Create an account, and then click on the “Datasets” tab at the top of the page.\n\n\n\n3. Filter for Texas\nSelect “TX” to view all of LegiScan’s data from the Texas Legislature. You’ll see their data goes back to 2010 (the 81st Texas Legislature). For now, we’re just going to focus on the 89th session, which is the current one for 2025-2026.\n\n\n\n4. Download the CSV file\nIn the top right corner of the data library is a column that says “CSV Basic.” This is the type of file we want to download.\n\n\n\n\n\n\nNote\n\n\n\nCSV stands for “comma separated values,” which is just one of many ways a dataset can be stored. This is a pretty common file type for working with data, so if you’re ever downloading data from other sources, you’ll likely see this as an option.\n\n\nClick on the red “CSV” button for the 89th session to download the file to your computer. It should be a compressed folder filled with multiple csv files.\n\n\n\n\n\n\n\nTip\n\n\n\nI recommend immediately renaming the folder to something that’s easy for you to find later. For example: “tx-bills-89”\n\n\n\n\n5. Take a look at the dataset",
    "crumbs": [
      "Sourcing the Data"
    ]
  },
  {
    "objectID": "charting.html",
    "href": "charting.html",
    "title": "Charting The Data",
    "section": "",
    "text": "Now that we’ve found some key stats from our analysis of legislative bills, we’re going to use data to make two different types of visualizations: a bar chart and a choropleth map. These are both very common ways of presenting data within news stories because they’re engaging, informative and visually pleasing.\nFor this lesson, you’re going to use the table of top bill sponsors that you saved in the previous step to build a bar chart. Then, you’ll use the other data set of bills filed per state senator to build the map. These are just two of many possible ways to visualize all the data you have at your fingertips.\n\nWorking in Datawrapper\nDatawrapper is a free tool that lets you upload or manually enter data and personalize the visualizations (charts, maps, and tables) with no coding required! It’s pretty user-friendly, and it produces clean, professional graphics similar to what you’d see on a news website.\n\nDatawrapper Academy\nDatawrapper has a great learning dashboard, called Datawrapper Academy, that walks you through how to design all the different types of visualizations on the website. I highly recommend you explore this resource, especially since we’re only going to walk through two types of graphics. Bookmark it and return to it the next time you want to build a chart for a news story or some other data project!\n\n\nOther tools for visualizations\nTableau Public and Flourish are also popular options for designing charts, but they’re a bit more complex in my opinion.\n\n\n\nWhat you will build\nTake a peek at the two finished charts below to get a sense of what you’re going to build.\n1. A bar chart of the top 10 bill sponsors from the session:\n\n2. An interactive map of Texas senate districts + bills filed per state senator:\n\n\n\n\n\n\nTip\n\n\n\nClick on a district to see the senator’s name and the number of bills/joint resolutions they sponsored this session. Also note the zoom feature in the lower right corner.\n\n\n\n\n\nCreate a Datawrapper account\nGo to datawrapper.de and click “Start creating” to make your free account. Once you’ve done that, you’re ready to start following along with the video tutorials and get designing!\n\n\nStep-by-step tutorials\n\nThe bar chart\nvideo tutorial for the first chart\n\n\nThe map\nvideo tutorial for the second chart",
    "crumbs": [
      "Visualizing the Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reporting on the Texas Legislature: A Data Journalism Toolkit",
    "section": "",
    "text": "An introduction to data journalism for political reporting including tips for sourcing raw data, using simple code for data analysis, and generating innovative storytelling ideas from the analysis.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analyzing The Data",
    "section": "",
    "text": "Before starting a data analysis, it’s a good idea to brain dump a series of questions you hope to answer based on the variables you know you’re working with. For this toolkit, we’re going to answer the following:\n\nWhich Texas lawmakers filed the most bills? What does that look like broken down by political party?\n\nWe’ll also look at who has filed the least\n\nFrom which districts do the lawmakers who have filed the most come from?\nWhat does the number of bills filed look like in each month? Which month had the most?\nHow many bills can we flag for some of the biggest policy issues in Texas right now: abortion, immigration, education, energy/environment, and transgender issues?\n\nWe’ll flag bills that use these terms explicitly, as well as bills that use related terms\n\nHow do the stats above compare to the 88th legislative session?\n\nThis is not an exhaustive list of the questions you could answer with this data, but it’s a good introduction to sorting through large data sets. Let’s take a look at these questions one at a time.\n\n1. Setup again\nAs with your cleaning file, you need to import the libraries that allow you to run certain functions. We’ll be loading the same ones. Copy the code below and run it.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(lubridate)\nlibrary(stringr)\n\n\n\n2. Importing the clean data\nCopy and run the code below to import the table we saved in the previous step. To view the table again, type “final_table” on a new line in the code chunk and run it again. There should be 14,190 rows of data with nine columns: people_id, name, party, role, district, bill_id, bill_number, date_filed, and description. Right now, you can see the data is listed in alphabetical order by lawmaker last name, which is why Rep. Alma Allen comes up for the first 50 rows. In the next few steps, we’ll use different functions to rearrange the table and look at the data in new ways!\n\nfinal_table &lt;- read_rds(\"data-processed/bills-89.rds\")\n\nfinal_table\n\n\n  \n\n\n\n\n\n3. Lawmakers: most vs. least filed\nNow let’s get into our first question: which lawmaker filed the most bills this session? To figure this out, we’re going to use three functions — group_by, summarize and arrange.\n\n“group_by” allows us to sort the data into specific groups based on variables that we want to focus on; in this case, we want to group by name, party, role, and district.\n“summarize” computes a summary of the data usually by a mean or a sum; we’ll just be focusing on the sum aspect because we want to count the number of bills each lawmaker has filed.\n“arrange” allows us to determine how we want to data to be presented; we can use this function to order the lawmaker names from most filed to least filed\n\nIt’s very common to use these three functions together to run basic computations of data. So let’s apply it to our data set! Run the code below to find out who filed the most bills.\n\n3.1 Doing the most first\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed))\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you try to run the code and you get an error, check the syntax. Do all of your indents match mine? Did you accidentally add in a comma or a parenthesis? Code syntax is extremely important, and most code errors are from very small mistakes in the typing.\n\n\nLooks like Sen. Bryan Hughes filed the most this session, with 276 bills. If you click through the table, you’ll see the values in the “number_filed” column decrease. That’s because we added the “desc()” element to our arrange function, which means we want it arranged in descending order.\n\n\n3.2 Just the top of the list\nWe’re looking at a lot of rows here, 180 to be exact, so what if we wanted to easily focus on the top 10 lawmakers who have filed the most this session? We can do this with the head() function. Run the code again, this time with one extra line at the end.\n\nmost_filed &lt;- final_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed)) |&gt; \n  head(10)\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\nmost_filed\n\n\n  \n\n\n\nBecause we put the number 10 inside the head function, the table now only shows the first 10 rows – or top 10 lawmakers based on number of bills filed. Feel free to mess around with the number inside the head function and see how your result changes.\n\n\n3.3 Now the least\nWe can easily see who filed the least bills with one small change to the code above. The arrange() function defaults to ascending order, so least filed to most filed, when we don’t have that desc() specification in the code. So all we need to do is take that out and run it!\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed)\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\n\n\n  \n\n\n\nRep. Ramon Romero Jr. filed the least this session, with only 10 bills. Like before, let’s just focus on the top 10 rows, which in this case would be the 10 lawmakers who filed the least bills this session. Copy and run the code below.\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed) |&gt; \n  head(10)\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\n\n\n  \n\n\n\nCongrats! You just answered the first questions from the brainstorm. And hopefully you’re starting to realize how you can organize a data table and look at it in different ways.\n\n\n\n4. Districts\nNow that we know who filed the most bills this session, we want to figure out which districts they represent. Our data set has a Texas house or senate district (HD/SD) associated with each lawmaker, so we can use a similar method as above to quickly see where the most “active” lawmakers come from.\n\nfinal_table |&gt; \n  group_by(name, district) |&gt; \n  count(district) |&gt; \n  arrange(desc(n))\n\n\n  \n\n\n\n\n\n5. Bills per month\n\nfinal_table |&gt; \n  group_by(status_date) |&gt; \n  summarise()\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNovember and March are both incomplete months in the data set because filing opened on Nov. 12 and closed on March 14. Keep this in mind when analyzing your results in this section.\n\n\n\n\n6. Policy topics // Status tracking\nTrying to organize status by type of legislation (HB, SB, HR, SR, HJR, SJR, HCR, SCR)\n\nsb_status &lt;- final_table |&gt; \n  filter(grepl(\"SB\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Bill\") \n\nhb_status &lt;- final_table |&gt; \n  filter(grepl(\"HB\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Bill\")\n\nsr_status &lt;- final_table |&gt; \n  filter(grepl(\"SR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Resolution\")\n\nhr_status &lt;- final_table |&gt; \n  filter(grepl(\"HR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Resolution\") \n\nhjr_status &lt;- final_table |&gt; \n  filter(grepl(\"HJR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Joint Res\")\n\nsjr_status &lt;- final_table |&gt; \n  filter(grepl(\"SJR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Joint Resolution\")\n\nhcr_status &lt;- final_table |&gt; \n  filter(grepl(\"HCR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Concurrent Resolution\")\n\nscr_status &lt;- final_table |&gt; \n  filter(grepl(\"SCR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Concurrent Resolution\")\n\n\nfinal_table |&gt; \n  filter(grepl(\"abortion\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"immigration\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"diversity\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"energy\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"cannabis|marijuana\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"prayer\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\n\n\n\n\n\nReminder\n\n\n\nThe data we’re using for this section comes from the bill description only, not the full bill text. This is important to keep in mind as we search for key words.\n\n\n\n\n7. This session vs. last session\n\n\n8. Saving tables for charts\n\nwrite_csv(most_filed, \"data-processed/most-filed.csv\")\n\n\n\n9. Recap of what we learned",
    "crumbs": [
      "Analyzing the Data"
    ]
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Cleaning The Data",
    "section": "",
    "text": "Some sort of introductory text to what cleaning data even means.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#setup",
    "href": "cleaning.html#setup",
    "title": "Cleaning The Data",
    "section": "1. Setup",
    "text": "1. Setup\nFirst, we need to load the packages that are going to help facilitate the data analysis. You don’t need to worry about what each package does, but this is an important setup step that allows us to use specific functions later on. I always include these three at the top of every R file.\n\n\n\n\n\n\nImportant\n\n\n\nAdd a “code chunk” to your file by hitting Command+Option+i on the keyboard. All of our code will be run within these chunks, so get used to this keyboard shortcut!\n\n\nCopy and paste the code below and “run” it by hitting the green play button in the top right of your code chunk.\n\n# setup \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(lubridate)\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll see some sort of message pop up after you run the code; as long as there’s no red “error” warning, you’re fine to move on.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#importing-the-data-from-legiscan",
    "href": "cleaning.html#importing-the-data-from-legiscan",
    "title": "Cleaning The Data",
    "section": "2. Importing the data from LegiScan",
    "text": "2. Importing the data from LegiScan\nNow you need to take the data that we downloaded to your computer and import it into this R file. It sounds complicated, but all it takes is a simple line of code.\n\n2.1 Create your data folders\nIf you open the files we downloaded from LegiScan in the previous step, you’ll see it actually includes seven different data tables. For the purposes of this project, we’re only going to be working with three of those: “bills,” “sponsors,” and “people.”\nCreate a new code chunk using the keyboard shortcut from above. Copy and run the code below to import those three tables into this file.\n\n bills &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/bills.csv\")\n\nRows: 9897 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): bill_number, status_desc, title, description, committee, last_acti...\ndbl  (4): bill_id, session_id, status, committee_id\ndate (2): status_date, last_action_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n bills\n\n\n  \n\n\n\n\nsponsors &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/sponsors.csv\")\n\nRows: 14190 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): bill_id, people_id, position\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsponsors\n\n\n  \n\n\n\n\nlawmakers &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/people.csv\")\n\nRows: 181 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): name, first_name, middle_name, last_name, suffix, nickname, party,...\ndbl  (7): people_id, party_id, role_id, followthemoney_eid, votesmart_id, kn...\nlgl  (1): opensecrets_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlawmakers",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#section",
    "href": "cleaning.html#section",
    "title": "Cleaning The Data",
    "section": "3.",
    "text": "3.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#creating-a-pretty-table-to-work-with",
    "href": "cleaning.html#creating-a-pretty-table-to-work-with",
    "title": "Cleaning The Data",
    "section": "3. Creating a pretty table to work with",
    "text": "3. Creating a pretty table to work with",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#cleaning-those-tables",
    "href": "cleaning.html#cleaning-those-tables",
    "title": "Cleaning The Data",
    "section": "3. “Cleaning” those tables",
    "text": "3. “Cleaning” those tables\nOftentimes, data tables aren’t in the best shape when you download them. There might be rows or cells that are missing entries, or there may be columns that aren’t necessary for the analysis you want to do. This is where cleaning comes in: you can use code to adjust the table/remove data you don’t need so the final table is much easier to work with.\n\n3.1 Bills\nLet’s clean up each of the data tables one at a time. We’ll start with our “bills” data.\n\nA) View the raw data\nBefore we start cleaning up the tables, let’s take another look at what they look like untouched. Run this code to view the table you just imported.\n\nbills \n\n\n  \n\n\n\nadd description of what we’re looking at here\n\n\nB) Remove unnecessary columns and rename columns\n\nbills_clean &lt;- bills |&gt; \n   select(-session_id, -url, -state_link) \n\nbills_clean \n\n\n  \n\n\n\n\n# add this to analysis file\nbills_clean |&gt; \n  group_by(status, status_desc) |&gt; \n  summarise(status_type = n()) |&gt; \n  arrange(desc(status_type))\n\n`summarise()` has grouped output by 'status'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\nDescribe what we’re looking at now.\n\n\n\n3.2 Lawmakers\n\nA) View the raw data\n\nlawmakers \n\n\n  \n\n\n\nDescription of what we’ve got here.\n\n\nB) Remove unnecessary columns and rename columns\n\nlawmakers_clean &lt;- lawmakers |&gt; \n  select(people_id, name, middle_name, suffix, party, role, district) \n\nlawmakers_clean\n\n\n  \n\n\n\nDescription of what we’re working with now.\n\n\n\n3.3 Sponsors\n\nA) View the raw data\n\nsponsors\n\n\n  \n\n\n\n\n\nB) Remove unnecessary columns and rename columns\n\nsponsors_clean &lt;- sponsors |&gt; \n  rename(spons_position = position)\n\nsponsors_clean\n\n\n  \n\n\n\nRestate all the changes made.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#saving-the-clean-table",
    "href": "cleaning.html#saving-the-clean-table",
    "title": "Cleaning The Data",
    "section": "5. Saving the clean table",
    "text": "5. Saving the clean table\nAdd instructions for creating a data-processed file.\n\nfinal_table |&gt; \n  write_rds(\"data-processed/bills-89.rds\")\n\nIn the next chapter of this toolkit, we’ll brain storm some potential questions about the session and use the data to find some answers.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#merge-the-tables-together",
    "href": "cleaning.html#merge-the-tables-together",
    "title": "Cleaning The Data",
    "section": "4. Merge the tables together",
    "text": "4. Merge the tables together\nNow that we’ve selected the columns we want to work with in each of the three data tables, we need to merge everything together into one big table that we’ll use for our analysis. You’ll notice there are a couple of repeat columns in the tables, such as people_id and bill_id. We’ll want to make sure those only appear once in our big table. Let’s do this in two steps: first, copy the code below to merge the lawmaker and sponsor tables together.\n\nlaw_spons &lt;- merge(lawmakers_clean, sponsors_clean) \n\nlaw_spons\n\n\n  \n\n\n\nNow lets add the description column and date_filed column from the bills table.\n\nfinal_table &lt;- law_spons |&gt; \n  left_join(bills_clean) |&gt; \n  rename(mid_init = middle_name) \n\nJoining with `by = join_by(bill_id)`\n\n\nGreat! You should have one big data table with the following information:\n\npeople_id = the unique number associated with each lawmaker\nname = lawmaker’s first and last name\nmid_init = lawmaker’s middle initial\nsuffix = lawmaker’s suffix, if applicable\nparty = political party of each lawmaker\nrole = whether the lawmaker is a senator or a representative\ndistrict = the congressional district associated with each lawmaker\nbill_id = the unique number associated with a specific bill filed this session\nbill_number = standard bill naming in the legislature\nspons_position = for a specific bill, this number indicates where a lawmaker is in the line of sponsor (ex: “1”=primary sponsor, “7”=seventh sponsor, etc.)\nstatus = a numerical representation of where the bill currently sits (ex: “0”=no progress, “1”=Introduced, “2”=Engrossed, “4”=Passed) There is no 3, I don’t know why.\nstatus_desc = written description of the status\nstatus_date = date a specific bill achieved the corresponding status\ntitle = shorter description of bill as written in LegiScan\ndescription = longer description of bill as written in LegiScan\ncommittee = which Senate/House committee the bill was referred to\ncommittee_id = unique code associated with each committee\nlast_action = most recent update to bill as of the date you downloaded the data\nlast_action_date = calendar date of that update\n\nThis is a lot of information to work with! We may not end up using every column, but it’s still important to have a full picture of the data available. The table has over 14,000 rows of data, each corresponding with a bill/joint resolution. Now you can see the appeal of sorting through everything in R, rather than manually or in a spreadsheet.\nAdd callout note about companion bills\nWe’ll be able to quickly sort through all of this information and identify key details about the agendas of Texas lawmakers in the current legislative session. Let’s save this table before we move forward with our analysis.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "about.html#a-credit-to-the-data-source",
    "href": "about.html#a-credit-to-the-data-source",
    "title": "About This Project",
    "section": "A credit to the data source",
    "text": "A credit to the data source\nInclude some sort of disclaimer that all data for this project came from LegiScan; link to site",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "about.html#about-the-author",
    "href": "about.html#about-the-author",
    "title": "About This Project",
    "section": "About the author",
    "text": "About the author\nThis is an independent project by Sarah Brager, a senior journalism student at The University of Texas at Austin. She created this project as her senior capstone for the Moody College Honors Program and UT’s Bridging Disciplines Program, where she’s also earning a certificate in public policy studies. Sarah is a data journalism fellow with UT’s Media Innovation Group. To learn more about Sarah and see some of her other work, view her LinkedIn at the top right of this website.",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "Sarah Brager",
    "section": "",
    "text": "Sarah Brager is a graduating senior at The University of Texas at Austin. She created this project as her senior honors capstone. She is currently a data journalism fellow with the UT Media Innovation Group, which is part of the Dallas Morning News Journalism Innovation Endowment.\nWith an interest in political and environmental reporting, Sarah plans to keep using data as an investigative tool in her career. To learn more about Sarah and view her other work, visit her LinkedIn and GitHub profiles.",
    "crumbs": [
      "Meet the Author"
    ]
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "Reporting on the Texas Legislature: A Data Journalism Toolkit",
    "section": "About this project",
    "text": "About this project",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#toolkit-objectives",
    "href": "index.html#toolkit-objectives",
    "title": "Reporting on the Texas Legislature: A Data Journalism Toolkit",
    "section": "Toolkit objectives",
    "text": "Toolkit objectives\n\nBy the end of this tutorial, you will have learned how to:",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "author.html#education",
    "href": "author.html#education",
    "title": "Sarah Brager",
    "section": "Education",
    "text": "Education\nThe University of Texas at Austin | Aug 2021 - May 2025\nBachelor of Journalism | Certificate in Public Policy Studies\nMoody College Honors Program | Bridging Disciplines Program",
    "crumbs": [
      "Meet the Author"
    ]
  },
  {
    "objectID": "author.html#about",
    "href": "author.html#about",
    "title": "Sarah Brager",
    "section": "",
    "text": "Sarah Brager is a graduating senior at The University of Texas at Austin. She created this project as her senior honors capstone. She is currently a data journalism fellow with the UT Media Innovation Group, which is part of the Dallas Morning News Journalism Innovation Endowment.\nWith an interest in political and environmental reporting, Sarah plans to keep using data as an investigative tool in her career. To learn more about Sarah and view her other work, visit her LinkedIn and GitHub profiles.",
    "crumbs": [
      "Meet the Author"
    ]
  }
]