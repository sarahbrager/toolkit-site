[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Resources if they want to learn more.",
    "crumbs": [
      "Additional Resources"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Intro to Data Journalism",
    "section": "",
    "text": "Data journalism, or data-driven storytelling, is a form of journalism where reporters/other members of a news organization sift through large data sets to uncover trends or other newsworthy information, and then present those findings to the public in a digestible way. Simply put, it’s telling the stories found in data. In reality, though, data journalism is much more creative and complex. It can take a variety of forms, including:\n\nDesigning visualizations and interactive news graphics to explain numbers (These can often be embedded within news articles.)\nUsing code to quickly sort through large data sets and pull out the most important information\nReporting investigative stories based on findings from a data analysis (You might find something important that’s been completely overlooked!)\nMachine learning and AI engineering for newsrooms (This method is more computer science-oriented.)\n\nExamples of data journalism:",
    "crumbs": [
      "Intro to Data Journalism"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Getting Started with R",
    "section": "",
    "text": "Tutorial on installing and setting up R workspace.",
    "crumbs": [
      "Setting Up"
    ]
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "reporting",
    "section": "",
    "text": "This will have markdown text with story suggestions and whatnot.",
    "crumbs": [
      "Reporting With This Data"
    ]
  },
  {
    "objectID": "sourcing.html",
    "href": "sourcing.html",
    "title": "Downloading Data from LegiScan",
    "section": "",
    "text": "Include a video tutorial at the top of this page.\nAll of the data for this tutorial comes from LegiScan, a real-time and nonpartisan legislative tracking service available to the public. While this project only focuses on data from the Texas Legislature, LegiScan has data sets from all 50 states. The data is updated weekly on Sundays.\nLegiScan is a good example of the type of public database that a data journalist might pull from frequently. There are lots of other resources available online with data from government agencies. If you’re a journalist covering the environment, for example, you might try xyz.\n\nSteps for downloading\n\n1. Go to legiscan.com\n\n\n2. Create a login\nLegiScan requires you to create a username and password to download datasets. This is completely free. Create an account, and then click on the “Datasets” tab at the top of the page.\n\n\n3. Filter for Texas\nSelect “TX” to view all of LegiScan’s data from the Texas Legislature. You’ll see their data goes back to 2010 (the 81st Texas Legislature). For now, we’re just going to focus on the 89th session, which is the current one for 2025-2026.\n\n\n4. Download the CSV file\nIn the top right corner of the data library is a column that says “CSV Basic.” This is the type of file we want to download.\n\n\n\n\n\n\nNote\n\n\n\nCSV stands for “comma separated values,” which is just one of many ways a dataset can be stored. This is a pretty common file type for working with data, so if you’re ever downloading data from other sources, you’ll likely see this as an option.\n\n\nClick on the red “CSV” button for the 89th session to download the file to your computer. It should be a compressed folder filled with multiple csv files.\n\n\n\n\n\n\nTip\n\n\n\nI recommend immediately renaming the folder to something that’s easy for you to find later. For example: “tx-bills-89”\n\n\n\n\n5. Take a look at the dataset",
    "crumbs": [
      "Sourcing the Data"
    ]
  },
  {
    "objectID": "charting.html",
    "href": "charting.html",
    "title": "Charting The Data",
    "section": "",
    "text": "This is a markdown with instructions for using Datawrapper to make three simple charts from the data analysis.",
    "crumbs": [
      "Making Simple News Graphics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reporting on the Texas Legislature: A Data Journalism Toolkit",
    "section": "",
    "text": "An introduction to data journalism for political reporting including tips for sourcing raw data, using simple code for data analysis, and generating innovative storytelling ideas from the analysis.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analyzing The Data",
    "section": "",
    "text": "Before starting a data analysis, it’s a good idea to brain dump a series of questions you hope to answer based on the variables you know you’re working with. For this toolkit, we’re going to answer the following:\n\nWhich Texas lawmakers filed the most bills? What does that look like broken down by political party?\n\nWe’ll also look at who has filed the least\n\nFrom which districts do the lawmakers who have filed the most come from?\nWhat does the number of bills filed look like in each month? Which month had the most?\nHow many bills can we flag for some of the biggest policy issues in Texas right now: abortion, immigration, education, energy/environment, and transgender issues?\n\nWe’ll flag bills that use these terms explicitly, as well as bills that use related terms\n\nHow do the stats above compare to the 88th legislative session?\n\nThis is not an exhaustive list of the questions you could answer with this data, but it’s a good introduction to sorting through large data sets. Let’s take a look at these questions one at a time.\n\n1. Setup again\nAs with your cleaning file, you need to import the libraries that allow you to run certain functions. We’ll be loading the same ones. Copy the code below and run it.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(lubridate)\nlibrary(stringr)\n\n\n\n2. Importing the clean data\nCopy and run the code below to import the table we saved in the previous step. To view the table again, type “final_table” on a new line in the code chunk and run it again. There should be 14,190 rows of data with nine columns: people_id, name, party, role, district, bill_id, bill_number, date_filed, and description. Right now, you can see the data is listed in alphabetical order by lawmaker last name, which is why Rep. Alma Allen comes up for the first 50 rows. In the next few steps, we’ll use different functions to rearrange the table and look at the data in new ways!\n\nfinal_table &lt;- read_rds(\"data-processed/bills-89.rds\")\n\nfinal_table\n\n\n  \n\n\n\n\n\n3. Lawmakers: most vs. least filed\nNow let’s get into our first question: which lawmaker filed the most bills this session? To figure this out, we’re going to use three functions — group_by, summarize and arrange.\n\n“group_by” allows us to sort the data into specific groups based on variables that we want to focus on; in this case, we want to group by name, party, role, and district.\n“summarize” computes a summary of the data usually by a mean or a sum; we’ll just be focusing on the sum aspect because we want to count the number of bills each lawmaker has filed.\n“arrange” allows us to determine how we want to data to be presented; we can use this function to order the lawmaker names from most filed to least filed\n\nIt’s very common to use these three functions together to run basic computations of data. So let’s apply it to our data set! Run the code below to find out who filed the most bills.\n\n3.1 Doing the most first\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed))\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you try to run the code and you get an error, check the syntax. Do all of your indents match mine? Did you accidentally add in a comma or a parenthesis? Code syntax is extremely important, and most code errors are from very small mistakes in the typing.\n\n\nLooks like Sen. Bryan Hughes filed the most this session, with 276 bills. If you click through the table, you’ll see the values in the “number_filed” column decrease. That’s because we added the “desc()” element to our arrange function, which means we want it arranged in descending order.\n\n\n3.2 Just the top of the list\nWe’re looking at a lot of rows here, 180 to be exact, so what if we wanted to easily focus on the top 10 lawmakers who have filed the most this session? We can do this with the head() function. Run the code again, this time with one extra line at the end.\n\nmost_filed &lt;- final_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed)) |&gt; \n  head(10)\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\nmost_filed\n\n\n  \n\n\n\nBecause we put the number 10 inside the head function, the table now only shows the first 10 rows – or top 10 lawmakers based on number of bills filed. Feel free to mess around with the number inside the head function and see how your result changes.\n\n\n3.3 Now the least\nWe can easily see who filed the least bills with one small change to the code above. The arrange() function defaults to ascending order, so least filed to most filed, when we don’t have that desc() specification in the code. So all we need to do is take that out and run it!\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed)\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\n\n\n  \n\n\n\nRep. Ramon Romero Jr. filed the least this session, with only 10 bills. Like before, let’s just focus on the top 10 rows, which in this case would be the 10 lawmakers who filed the least bills this session. Copy and run the code below.\n\nfinal_table |&gt; \n  group_by(name, party, role, district) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed) |&gt; \n  head(10)\n\n`summarise()` has grouped output by 'name', 'party', 'role'. You can override\nusing the `.groups` argument.\n\n\n\n  \n\n\n\nCongrats! You just answered the first questions from the brainstorm. And hopefully you’re starting to realize how you can organize a data table and look at it in different ways.\n\n\n\n4. Districts\nNow that we know who filed the most bills this session, we want to figure out which districts they represent. Our data set has a Texas house or senate district (HD/SD) associated with each lawmaker, so we can use a similar method as above to quickly see where the most “active” lawmakers come from.\n\nfinal_table |&gt; \n  group_by(name, district) |&gt; \n  count(district) |&gt; \n  arrange(desc(n))\n\n\n  \n\n\n\n\n\n5. Bills per month\n\nfinal_table |&gt; \n  group_by(status_date) |&gt; \n  summarise()\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNovember and March are both incomplete months in the data set because filing opened on Nov. 12 and closed on March 14. Keep this in mind when analyzing your results in this section.\n\n\n\n\n6. Policy topics // Status tracking\nTrying to organize status by type of legislation (HB, SB, HR, SR, HJR, SJR, HCR, SCR)\n\nsb_status &lt;- final_table |&gt; \n  filter(grepl(\"SB\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Bill\") \n\nhb_status &lt;- final_table |&gt; \n  filter(grepl(\"HB\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Bill\")\n\nsr_status &lt;- final_table |&gt; \n  filter(grepl(\"SR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Resolution\")\n\nhr_status &lt;- final_table |&gt; \n  filter(grepl(\"HR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Resolution\") \n\nhjr_status &lt;- final_table |&gt; \n  filter(grepl(\"HJR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Joint Res\")\n\nsjr_status &lt;- final_table |&gt; \n  filter(grepl(\"SJR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Joint Resolution\")\n\nhcr_status &lt;- final_table |&gt; \n  filter(grepl(\"HCR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"House\", bill_type = \"Concurrent Resolution\")\n\nscr_status &lt;- final_table |&gt; \n  filter(grepl(\"SCR\", bill_number, ignore.case = TRUE)) |&gt; \n  count(status_desc) |&gt; \n  mutate(chamber = \"Sen\", bill_type = \"Concurrent Resolution\")\n\n\nfinal_table |&gt; \n  filter(grepl(\"abortion\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"immigration\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"diversity\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"energy\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"cannabis|marijuana\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\nfinal_table |&gt; \n  filter(grepl(\"prayer\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\n\n\n\n\n\nReminder\n\n\n\nThe data we’re using for this section comes from the bill description only, not the full bill text. This is important to keep in mind as we search for key words.\n\n\n\n\n7. This session vs. last session\n\n\n8. Saving tables for charts\n\nwrite_csv(most_filed, \"data-processed/most-filed.csv\")\n\n\n\n9. Recap of what we learned",
    "crumbs": [
      "Analyzing the Data"
    ]
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Cleaning The Data",
    "section": "",
    "text": "Some sort of introductory text to what cleaning data even means.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#setup",
    "href": "cleaning.html#setup",
    "title": "Cleaning The Data",
    "section": "1. Setup",
    "text": "1. Setup\nFirst, we need to load the packages that are going to help facilitate the data analysis. You don’t need to worry about what each package does, but this is an important setup step that allows us to use specific functions later on. I always include these three at the top of every R file.\n\n\n\n\n\n\nImportant\n\n\n\nAdd a “code chunk” to your file by hitting Command+Option+i on the keyboard. All of our code will be run within these chunks, so get used to this keyboard shortcut!\n\n\nCopy and paste the code below and “run” it by hitting the green play button in the top right of your code chunk.\n\n# setup \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(lubridate)\n\n\n\n\n\n\n\nTip\n\n\n\nYou’ll see some sort of message pop up after you run the code; as long as there’s no red “error” warning, you’re fine to move on.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#importing-the-data-from-legiscan",
    "href": "cleaning.html#importing-the-data-from-legiscan",
    "title": "Cleaning The Data",
    "section": "2. Importing the data from LegiScan",
    "text": "2. Importing the data from LegiScan\nNow you need to take the data that we downloaded to your computer and import it into this R file. It sounds complicated, but all it takes is a simple line of code.\n\n2.1 Create your data folders\nIf you open the files we downloaded from LegiScan in the previous step, you’ll see it actually includes seven different data tables. For the purposes of this project, we’re only going to be working with three of those: “bills,” “sponsors,” and “people.”\nCreate a new code chunk using the keyboard shortcut from above. Copy and run the code below to import those three tables into this file.\n\n bills &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/bills.csv\")\n\nRows: 9897 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): bill_number, status_desc, title, description, committee, last_acti...\ndbl  (4): bill_id, session_id, status, committee_id\ndate (2): status_date, last_action_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n bills\n\n\n  \n\n\n\n\nsponsors &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/sponsors.csv\")\n\nRows: 14190 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): bill_id, people_id, position\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsponsors\n\n\n  \n\n\n\n\nlawmakers &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/people.csv\")\n\nRows: 181 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): name, first_name, middle_name, last_name, suffix, nickname, party,...\ndbl  (7): people_id, party_id, role_id, followthemoney_eid, votesmart_id, kn...\nlgl  (1): opensecrets_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlawmakers",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#section",
    "href": "cleaning.html#section",
    "title": "Cleaning The Data",
    "section": "3.",
    "text": "3.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#creating-a-pretty-table-to-work-with",
    "href": "cleaning.html#creating-a-pretty-table-to-work-with",
    "title": "Cleaning The Data",
    "section": "3. Creating a pretty table to work with",
    "text": "3. Creating a pretty table to work with",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#cleaning-those-tables",
    "href": "cleaning.html#cleaning-those-tables",
    "title": "Cleaning The Data",
    "section": "3. “Cleaning” those tables",
    "text": "3. “Cleaning” those tables\nOftentimes, data tables aren’t in the best shape when you download them. There might be rows or cells that are missing entries, or there may be columns that aren’t necessary for the analysis you want to do. This is where cleaning comes in: you can use code to adjust the table/remove data you don’t need so the final table is much easier to work with.\n\n3.1 Bills\nLet’s clean up each of the data tables one at a time. We’ll start with our “bills” data.\n\nA) View the raw data\nBefore we start cleaning up the tables, let’s take another look at what they look like untouched. Run this code to view the table you just imported.\n\nbills \n\n\n  \n\n\n\nadd description of what we’re looking at here\n\n\nB) Remove unnecessary columns and rename columns\n\nbills_clean &lt;- bills |&gt; \n   select(-session_id, -url, -state_link) \n\nbills_clean \n\n\n  \n\n\n\n\n# add this to analysis file\nbills_clean |&gt; \n  group_by(status, status_desc) |&gt; \n  summarise(status_type = n()) |&gt; \n  arrange(desc(status_type))\n\n`summarise()` has grouped output by 'status'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\nDescribe what we’re looking at now.\n\n\n\n3.2 Lawmakers\n\nA) View the raw data\n\nlawmakers \n\n\n  \n\n\n\nDescription of what we’ve got here.\n\n\nB) Remove unnecessary columns and rename columns\n\nlawmakers_clean &lt;- lawmakers |&gt; \n  select(people_id, name, middle_name, suffix, party, role, district) \n\nlawmakers_clean\n\n\n  \n\n\n\nDescription of what we’re working with now.\n\n\n\n3.3 Sponsors\n\nA) View the raw data\n\nsponsors\n\n\n  \n\n\n\n\n\nB) Remove unnecessary columns and rename columns\n\nsponsors_clean &lt;- sponsors |&gt; \n  rename(spons_position = position)\n\nsponsors_clean\n\n\n  \n\n\n\nRestate all the changes made.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#saving-the-clean-table",
    "href": "cleaning.html#saving-the-clean-table",
    "title": "Cleaning The Data",
    "section": "5. Saving the clean table",
    "text": "5. Saving the clean table\nAdd instructions for creating a data-processed file.\n\nfinal_table |&gt; \n  write_rds(\"data-processed/bills-89.rds\")\n\nIn the next chapter of this toolkit, we’ll brain storm some potential questions about the session and use the data to find some answers.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#merge-the-tables-together",
    "href": "cleaning.html#merge-the-tables-together",
    "title": "Cleaning The Data",
    "section": "4. Merge the tables together",
    "text": "4. Merge the tables together\nNow that we’ve selected the columns we want to work with in each of the three data tables, we need to merge everything together into one big table that we’ll use for our analysis. You’ll notice there are a couple of repeat columns in the tables, such as people_id and bill_id. We’ll want to make sure those only appear once in our big table. Let’s do this in two steps: first, copy the code below to merge the lawmaker and sponsor tables together.\n\nlaw_spons &lt;- merge(lawmakers_clean, sponsors_clean) \n\nlaw_spons\n\n\n  \n\n\n\nNow lets add the description column and date_filed column from the bills table.\n\nfinal_table &lt;- law_spons |&gt; \n  left_join(bills_clean) |&gt; \n  rename(mid_init = middle_name) \n\nJoining with `by = join_by(bill_id)`\n\n\nGreat! You should have one big data table with the following information:\n\npeople_id = the unique number associated with each lawmaker\nname = lawmaker’s first and last name\nmid_init = lawmaker’s middle initial\nsuffix = lawmaker’s suffix, if applicable\nparty = political party of each lawmaker\nrole = whether the lawmaker is a senator or a representative\ndistrict = the congressional district associated with each lawmaker\nbill_id = the unique number associated with a specific bill filed this session\nbill_number = standard bill naming in the legislature\nspons_position = for a specific bill, this number indicates where a lawmaker is in the line of sponsor (ex: “1”=primary sponsor, “7”=seventh sponsor, etc.)\nstatus = a numerical representation of where the bill currently sits (ex: “0”=no progress, “1”=Introduced, “2”=Engrossed, “4”=Passed) There is no 3, I don’t know why.\nstatus_desc = written description of the status\nstatus_date = date a specific bill achieved the corresponding status\ntitle = shorter description of bill as written in LegiScan\ndescription = longer description of bill as written in LegiScan\ncommittee = which Senate/House committee the bill was referred to\ncommittee_id = unique code associated with each committee\nlast_action = most recent update to bill as of the date you downloaded the data\nlast_action_date = calendar date of that update\n\nThis is a lot of information to work with! We may not end up using every column, but it’s still important to have a full picture of the data available. The table has over 14,000 rows of data, each corresponding with a bill/joint resolution. Now you can see the appeal of sorting through everything in R, rather than manually or in a spreadsheet.\nAdd callout note about companion bills\nWe’ll be able to quickly sort through all of this information and identify key details about the agendas of Texas lawmakers in the current legislative session. Let’s save this table before we move forward with our analysis.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "about.html#a-credit-to-the-data-source",
    "href": "about.html#a-credit-to-the-data-source",
    "title": "About This Project",
    "section": "A credit to the data source",
    "text": "A credit to the data source\nInclude some sort of disclaimer that all data for this project came from LegiScan; link to site",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "about.html#about-the-author",
    "href": "about.html#about-the-author",
    "title": "About This Project",
    "section": "About the author",
    "text": "About the author\nThis is an independent project by Sarah Brager, a senior journalism student at The University of Texas at Austin. She created this project as her senior capstone for the Moody College Honors Program and UT’s Bridging Disciplines Program, where she’s also earning a certificate in public policy studies. Sarah is a data journalism fellow with UT’s Media Innovation Group. To learn more about Sarah and see some of her other work, view her LinkedIn at the top right of this website.",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "Meet the Author",
    "section": "",
    "text": "Sarah Brager is a senior journalism student at The University of Texas at Austin. She created this project as her senior capstone for the Moody College Honors Program and Bridging Disciplines Program at UT, where she’s also earning a certificate in public policy studies. She is currently a data journalism fellow with UT’s Media Innovation Group. To learn more about Sarah and view her other work, visit her LinkedIn at the top right of this website.",
    "crumbs": [
      "Meet the Author"
    ]
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "Reporting on the Texas Legislature: A Data Journalism Toolkit",
    "section": "About this project",
    "text": "About this project",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#toolkit-objectives",
    "href": "index.html#toolkit-objectives",
    "title": "Reporting on the Texas Legislature: A Data Journalism Toolkit",
    "section": "Toolkit objectives",
    "text": "Toolkit objectives\n\nBy the end of this tutorial, you will have learned how to:",
    "crumbs": [
      "Home"
    ]
  }
]