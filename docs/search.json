[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This will have a list of resources related to data analysis/coding if people want to learn more than what’s in this toolkit.\n\nvideo for installing R\nLinks to R tutorials\nCode academy link again\nDatawrapper academy link again\nLink to spreadsheet tutorials",
    "crumbs": [
      "Additional Resources"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Intro to Data Journalism",
    "section": "",
    "text": "What is it?\nData journalism, or data-driven storytelling, is a form of journalism that involves sifting through large data sets to uncover trend and answer questions, and then presenting those findings to the public as you would any other type of newsworthy information. Simply put, it’s telling stories found in data.\nData-driven storytelling can be hard-hitting and investigative or fun and entertaining. There’s no one right way of data reporting, and it’s always changing as digital tools become more advanced and accessible. Data journalism has many forms, including:\n\nDesigning informative and interactive visualizations to embed within news stories\nUsing code to quickly sort through large data sets and pull out the most important information\nReporting investigative stories based on findings from a data analysis (You might find something important that’s been completely overlooked!)\nMachine learning and AI engineering for newsrooms (A more computer science-oriented method)\n\n\n\nExamples of data reporting\nMany news publications have specific data departments that focus on investigations, analytics and visualizations, including (but not limited to):\n\nProPublica\nThe New York Times\nThe Washington Post\nThe Texas Tribune\nThe Houston Chronicle\nThe Financial Times\nNBC News\n\n\n\nWhy is it useful?\nData-driven stories help people better grasp the impact of the news they’re consuming. Imagine you’re a journalist trying to explain a new policy measure, public health trend, crime spike, or education funding initiatve (all very common issues in the news) — each of these topics can be connected to some source of data … and that’s just a short list of examples. By running simple data analyses, you can beef up your reporting with more context, evidence, and visuals. Plus, it makes you a more credible journalist and helps you builds trust with your audience.\nIf your beat requires you to frequently work with public records, data skills can help! Government agencies sometimes send data in large, messy spreadsheets that seem impossible to sort through. If you requested information that spans years, for example, you might be given thousands of rows of information. Simple computing skills like the ones you’ll learn today can help you quickly sort through it and answer your reporting questions.\n\n\nLooking ahead\nNervous about working in R? The next section will break down some of the setup and help you familiarize yourself with the interface. It can be confusing and overwhelming at first, but once you get used to the style, it’s an extremely helpful and powerful resource for reporters.",
    "crumbs": [
      "What is Data Journalism?"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Getting Started with R",
    "section": "",
    "text": "Video tutorial on setting up workspace in posit.cloud.\n\nWhat is R?\nThis tutorial will introduce you to basic data analysis that can be done in R, which is one of many programming languages used by data journalists to better understand large, complex data sets. While this toolkit will show you some of R’s capabilities, it is by no means a complete look at how R can be used. If you like the activities in this toolkit, I highly recommend you check out the additional resources provided on this website.\n\nDo I need coding experience to use this toolkit?\nNope! An interest in stats is helpful, but this tutorial is designed for people who are new to data analysis and want an example of how news reporters can use data in their work. It may take a minute to adjust to the interface and style of R, but this lesson keeps things simple and user-friendly.\n\n\nAdditional resources\nIf you decide after this tutorial that you want to learn more R programming skills, check out these free R courses on Code Academy. We’ll go over some of those skills in this toolkit, but not all of them. If you want to build on what you’ll learn today, check out the Code Academy library (you can earn certifications for your resume!)\n\n\n\nFirst steps\n\n1. Go to posit.cloud and create an account\nThis is an online version of R, meaning you’ll be able to work with the same interface as the R desktop application but in your web browser. If you already have RStudio installed on your computer or would prefer to use the desktop version, that will also work.\n\n\nInstalling RStudio (optional)\nIf you want to install RStudio on your computer, instructions for that can be found on the Additional Resources tab on the left sidebar. The steps I provide throughout this toolkit are more specific to posit.cloud; that being said, if you already know how to set up projects within RStudio, feel free to run your analysis in there.\n\n\n\n\n\n\nTip\n\n\n\nIf this is your first time using R, or you’re not super comfortable with it, I highly recommend you do this activity in posit.cloud. It will be easier to follow along with the tutorial.\n\n\n\n\n2. Create a new project within posit.cloud\nOnce you’re logged in to your account, you should see a dashboard that lists all of your content (this is empty at the moment, of course). In the top right corner, click on the drop down button the create a new project and select “New RStudio Project.” All of the work you’ll do for this toolkit will be within this project, so if you leave and come back to these instructions make sure to re-enter the same project through your dashboard.\n\n\n\n\n\n\n\n3. Name your project/familiarize yourself with the interface\nNow that you’re inside the project, immediately give it a name at the top of the workspace. I recommend something short and related to the data we’ll be working with, like “tx-lege-toolkit”\n\nNow you can see what the RStudio workspace looks like! It can be a a lot to take in at first, but you’ll adjust to the layout once you start creating files and working in them. Click on the boxes shown in the photo above the collapse the console, creating a fourth window where your files will eventually appear.\nThese are the 4 main windows within any R project: file workspace (top left), console/terminal (bottom left), environment (top right), and file list (bottom right). You can adjust the size of each window by hovering your cursor between them.\n\n\n\n4. Adding to your library\n\nFirst, folders\nBefore we download the data, let’s create two folders where you’ll drop the files. This is a good practice for keeping your files organized in any R project you work in. In your files window (bottom right), click the icon in the top left corner to create and name a new folder. Title the first one “data-raw” and then create a second folder called “data-processed”\n\nOnce you’ve created those, you’ll see them appear in your files list. The data-raw folder is where you’ll eventually put the unedited dataset that we’re going to download in the next step. Any data that you clean up/edit should be stored in the data-processed folder. It’s always good to keep a copy of the original data separate from the version that we’re going to edit and analyze, hence the two folders. Keep this in mind for any future data projects you decide to pursue!\n\n\nNow, the Quarto files\nWe’re also going to create two files inside which we’ll run code to analyze the data. Click the new file icon and select “Quarto Document.” Name one “cleaning.qmd” and then create another and call it “analysis.qmd.” Both of those should appear in your files list. We’ll come back to these later on in the tutorial.\n\n\n\n\n\nRecap\nIn this section, you learned how to create a new project in posit.cloud and R Studio and add files within that project. You should have two folders, data-raw and data-processed, and two files, one for cleaning and one for analysis.\nNow that you’ve set up your project, you’re ready to start working with the data! Feel free to return to these setup instructions for any other project you decide to create in posit.cloud or the R Studio app.\nIn the next section, we’re going to download the dataset of bills from the Texas Legislative session and add that data to your R project.",
    "crumbs": [
      "Setting Up"
    ]
  },
  {
    "objectID": "reporting.html",
    "href": "reporting.html",
    "title": "Using the Data as a Journalist",
    "section": "",
    "text": "intro\n\nIncorporating the data in your reporting\nIf you’re a journalist covering Texas politics and policymaking in any way, these statistics could provide great context for readers about the current legislative session. Whether you’re doing a broad overview of trends from the session or reporting on a specific bill, you can come back to this analysis and pull information to weave into your article. For example:\n\nSome tips for writing with numbers\n\nDon’t include more than 2-3 stats in a paragraph.\n\n\n\n\nWhat if I’m a journalist in a different state?\nYou can run this exact analysis using LegiScan data from any other state! Follow the instructions on the Sourcing page, but select your state of choice when downloading the data.\n\n\n\nStory ideas based on the data",
    "crumbs": [
      "Reporting with the Data"
    ]
  },
  {
    "objectID": "sourcing.html",
    "href": "sourcing.html",
    "title": "Downloading Data from LegiScan",
    "section": "",
    "text": "Include a video tutorial at the top of this page\n\nOur data source\nAll of the data for this tutorial comes from LegiScan, a real-time and nonpartisan legislative tracking service available to the public. While this project only focuses on data from the Texas Legislature, LegiScan offers data sets from all 50 states. The data is updated weekly on Sundays.\nLegiScan is a good example of the type of public database that a data journalist might pull from frequently. There are lots of other resources available online with data from government agencies. If you’re a journalist covering the environment, for example, you might try the EPA’s Open Data Portal. If you’re a local government reporter, you might pull public records from your city’s database, like the City of Austin’s portal. And then there are more general community databases, like Kaggle, which lets you filter by topic.\nNow let’s get into the Texas Lege data!\n\n\nSteps for downloading\n\n1. Go to legiscan.com\n\n\n2. Create an account\nLegiScan requires you to create a username and password to download datasets. This is completely free. Create an account, and then click on the “Datasets” tab at the top of the page.\n\n\n\n\n\n\nNote\n\n\n\nLegiScan will ask you to verify your account before you can download the data. Make sure you do that.\n\n\n\n\n\n3. Filter for Texas\nSelect “TX” to view all of LegiScan’s data from the Texas Legislature. You’ll see their data goes back to 2010 (the 81st Texas Legislature). For now, we’re just going to focus on the 89th session, which is the current one for 2025-2026.\n\n\n\n4. Download the CSV file\nIn the top right corner of the data library is a column that says “CSV Basic.” This is the type of file we want to download.\n\n\n\n\n\n\nNote\n\n\n\nCSV stands for “comma separated values,” which is just one of many ways a dataset can be stored. This is a pretty common file type for working with data, so if you’re ever downloading data from other sources, you’ll likely see this as an option.\n\n\nClick on the red “CSV” button for the 89th session to download the file to your computer. It should be a compressed folder filled with multiple csv files.\n\n\n\n\n\n\n\nTip\n\n\n\nI recommend immediately renaming the folder to something that makes sense for the topic and is easy to find later on your computer. For example: “tx-bills-89”\n\n\n\n\n\nTake a look at the data\nOpen the zip file that was downloaded to your computer and take a look at the different files inside. Notice that you actually get 7 different CSVs when you download from LegiScan. You can see when the data was last updated by looking at the Date Modified column. No need to go through each file now, we’ll start looking at those in the next section.\n\n\n\nAdd the files to your R project\nGo back to your project in posit.cloud and click on the Upload button in the bottom right window.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re working in the R Studio application, there isn’t a little upload button within the R interface. Instead, you can manually move the CSVs to your data-raw folder in Finder (File Explorer if you use a PC).\n\n\nA little window will pop up asking you to choose the files from your computer. In this tutorial, we’re just going to use three of the CSVs: “bills,” “people,” and “sponsors.” Add those to your project one at a time:\n\nClick on Choose File and select one of the CSVs.\nWhere it says target directory, click on browse and select your data-raw folder (to keep things organized). Hit OK.\nDo the same thing for the second and third CSV files.\nAdd the README file to your project as well. We’ll take a look at that in the next section. No need to add this one to the data-raw folder.\n\n\nNow you should see all three datasets inside your data-raw folder when you click on it. Make sure they are actually inside the folder; this will prevent errors in the next section of the lesson. To exit out of the folder, click on the two little dots next to the green arrow.\n\n\nRecap\nIn this section, you learned how to download data from LegiScan, view the files on your computer, and upload those files to your R project. These are universal steps you can follow for any data project in posit.cloud/R Studio, so keep this page in mind as a resource.\nNow you’re ready to start coding! In the next section, we’ll look at all of the data provided and “clean it” in R to pick out the specific variables we want to focus on for this project.",
    "crumbs": [
      "Sourcing the Data"
    ]
  },
  {
    "objectID": "charting.html",
    "href": "charting.html",
    "title": "Making Data Visualizations",
    "section": "",
    "text": "Now that we’ve found some key stats from our analysis of legislative bills, we’re going to use data to make two different types of visualizations: a bar chart and a choropleth map. These are both very common ways of presenting data within news stories because they’re engaging, informative and visually pleasing.\nFor this lesson, you’re going to use the table of top bill sponsors that you saved in the previous step to build a bar chart. Then, you’ll use the other data set of bills filed per state senator to build the map. These are just two of many possible ways to visualize all the data you have at your fingertips.\n\nWorking in Datawrapper\nDatawrapper is a free tool that lets you upload or manually enter data and personalize the visualizations (charts, maps, and tables) with no coding required! It’s pretty user-friendly, and it produces clean, professional graphics similar to what you’d see on a news website.\n\nDatawrapper Academy\nDatawrapper has a great learning dashboard, called Datawrapper Academy, that walks you through how to design all the different types of visualizations on the website. I highly recommend you explore this resource, especially since we’re only going to walk through two types of graphics. Bookmark it and return to it the next time you want to build a chart for a news story or some other data project!\n\n\nOther tools for visualizations\nTableau Public and Flourish are also popular options for designing charts, but they’re a bit more complex in my opinion.\n\n\n\nWhat you will build\nTake a peek at the two finished charts below to get a sense of what you’re going to build.\n1. A bar chart of the top 10 bill sponsors from the session:\n\n2. An interactive map of Texas senate districts + bills filed per state senator:\n\n\n\n\n\n\nTip\n\n\n\nClick on a district to see the senator’s name and the number of bills/joint resolutions they sponsored this session. Also note the zoom feature in the lower right corner.\n\n\n\n\n\nCreate a Datawrapper account\nGo to datawrapper.de and click “Start creating” to make your free account. Once you’ve done that, you’re ready to start following along with the video tutorials and get designing!\n\n\nStep-by-step videos\n\nThe bar chart\nvideo tutorial for the first chart\n\n\nThe map\nvideo tutorial for the second chart\n\n\n\nRecap",
    "crumbs": [
      "Visualizing the Data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reporting on the Texas Legislature Using Data: A Digital Toolkit for Journalists",
    "section": "",
    "text": "This toolkit offers an introduction to data analysis for news reporting, with step-by-step tutorials for downloading data, analyzing it in R, making different visualizations, and brainstorming potential news stories based on the findings. All of the exercises in this lesson center around one dataset of Texas legislative bills from the 89th session, but the skills taught throughout the tutorial can apply to other data analysis projects.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analyzing The Data",
    "section": "",
    "text": "Before I start any analysis, I like to brain dump a few questions I hope to answer based on the data I’m working with. This gives me a sense of direction for which computations to run, and it helps me keep track of my work if I ever return to old analysis projects. In this lesson, we’re going to answer the following:\n\nWhich Texas lawmakers sponsored the most legislation? Who sponsored the least legislation?\n\nWe’ll also break this down by political party and compare Democrats vs. Republicans.\n\nLooking at the top 10 sponsors of this session: Which parts of the state do they represent?\nWhat does the distribution of bill status look like? (Introduced vs. Engrossed vs. Passed)\n\nHow many haven’t made any progress at all?\n\nHow can we identify bills based on keywords, like “immigration” to name one example?\nHow many bills have been assigned to each committee? How many committees make up the Texas Senate vs. the House?\n\nThis is not an exhaustive list of the questions you could answer with this data, but it’s a good introduction to sorting through large data sets. Let’s take a look at these questions one at a time.\n\n\n\n\n\n\nNote\n\n\n\nMy results might look different than yours throughout this section because I downloaded the data on a different date, meaning yours is more up-to-date than mine. If you ever re-run this analysis at a later date, expect your results to change.\n\n\n\nSetup again\nOpen your analysis.qmd file.\nAs with your cleaning file, you need to load the libraries that allow you to run certain functions. We’ll be using the same ones. Type out the code below and run it in a new block at the top of the file.\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\n\n\n\nImportant\n\n\n\nDon’t forget to label your work and save regularly!\n\n\n\n\nImport the clean data\nRemember that rds file you saved in the previous section? Now we want to tell R to read that file so it appears here. To do so, you’ll use a very similar function to read_csv() … which is called read_rds(). As with read_csv, you’ll write a direct path to where the rds file sits in your data-processed folder.\nRun read_rds() for the processed data, and save it in an object called “final_table.” After you’ve run that command, “call” the object final_table (meaning run it) so the data appears.\nIf you need a reminder, the code looks like this:\n\nfinal_table &lt;- read_rds(\"data-processed/bills-89.rds\")\n\n\nfinal_table\n\n\n  \n\n\n\nIf you click through, it should be the exact table you saved at the end of the cleaning process. Right now, you can see the data appears in alphabetical order by lawmaker last name, which is why Rep. Alma Allen comes up for the first 50 rows.\nIn the next few steps, we’ll use different functions to rearrange the table and look at the data in new ways!\n\n\n1. Sponsors: most vs. least\nNow let’s get into the first question: Who sponsored the most bills this session? To figure this out, youll use three functions — group_by(), summarize() and arrange().\n\ngroup_by() allows us to sort the data into specific groups based on variables that we want to focus on; in this case, we want to group by name, party, role, and district.\nsummarize() computes a summary of the data usually by a mean or a sum; we’ll just be focusing on the sum aspect because we want to count the number of bills each lawmaker has filed.\narrange() allows us to determine how we want to data to be presented; we can use this function to order the lawmaker names from most filed to least filed\n\nIt’s very common to use these three functions together to run simple computations, so get used to this order of operations.\n\n1.1 Doing the most first\nRun the following code to see who sponsored the most bills this session.\n\nfinal_table |&gt; \n  group_by(name, party, role) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed))\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you run the code and you get an error, check the syntax. Do all of your indents match mine? Did you accidentally add in a comma or a parenthesis? Code syntax is extremely important, and most code errors are from very small mistakes in the typing.\n\n\nLooks like Sen. Bryan Hughes sponsored the most this session. If you click through the table, you’ll see the values in the “number_filed” column decrease. That’s because we added the “desc()” element to our arrange function, which means we want it arranged in descending order.\n\n\n\n\n\n\nNote\n\n\n\nMy exact numbers might look different from yours because I downloaded this data on a different day, so don’t worry if your result isn’t exactly the same as mine.\n\n\n\n\n1.2 Just the top of the list\nWe’re looking at a lot of rows here, 180 to be exact, so what if we wanted to highlight the top 10 lawmakers who have sponsored the most this session? We can do this with the head() function. Run the code again, this time with one extra line at the end. Save it into an object called most_spons.\n\nmost_spons &lt;- final_table |&gt; \n  group_by(name, party, role) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(desc(number_filed)) |&gt; \n  head(10)\n\nNow call most_spons to see the top 10 list.\n\nmost_spons\n\n\n  \n\n\n\nBecause we put the number 10 inside the head function, the table now only shows the first 10 rows – or top 10 lawmakers based on number of bills filed. Feel free to mess around with the number inside the head function and see how your result changes.\n\n\n1.3 Who sponsored the least?\nWe can easily see who filed the least bills with one small change to the code above. The arrange() function defaults to ascending order when we don’t have that desc() specification in the code. So all we need to do is take that out and run it! Do this in a new code block, not the one you used earlier.\n\nfinal_table |&gt; \n  group_by(name, party, role) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed)\n\n\n  \n\n\n\nRep. Ramon Romero Jr. sponsored the least legislation this session, with only 10 bills. Like before, let’s just focus on the top 10 rows, which in this case would be the 10 lawmakers who filed the least bills this session. I think you know what to add here!\n\nleast_spons &lt;- final_table |&gt; \n  group_by(name, party, role) |&gt; \n  summarise(number_filed = n()) |&gt; \n  arrange(number_filed) |&gt; \n  head(10)\n\n\nleast_spons\n\n\n  \n\n\n\n\n\n1.4 Democrats vs. Republicans\nTo break this down by political party, we’re going to use the filter() function, which tells R to filter for a certain condition in the data. In this case, the condition is whether party equals D or R.\nFirst we’re going to tell it to filter for Democratic lawmakers by setting party == to “D” (note the double equal sign, this is the proper syntax). Then, we’re telling it to count the number of times party equals D. That tells us the number of bills/resolutions sponsored by a Democratic lawmaker.\nCopy and run the code.\n\nfinal_table |&gt; \n  filter(party == \"D\") |&gt; \n  count(party)\n\n\n  \n\n\n\nRepeat this in a new chunk, but filter for Republicans. Try writing it on your own first before viewing my code.\n\n\nCode\nfinal_table |&gt; \n  filter(party == \"R\") |&gt; \n  count(party)\n\n\n\n  \n\n\n\nYou can see 8,969 bills/resolutions were sponsored by Republicans, while only 5,221 were sponsored by Democrats. This makes sense, considering Republicans have the majority in Congress.\nCongrats! You just answered the first questions from the brainstorm. And hopefully you’re starting to realize how you can organize a data table and look at it in different ways.\n\n\n\n2. Districts\nNow that we know who filed the most bills this session, we want to figure out which districts they represent. Our data set has a Texas house or senate district (HD/SD) associated with each lawmaker, so we can use a similar method as above to see where each one comes from. Key things here:\n\nWe’re adding the district column to the group_by list\nWe’re using the count() function to tell R to count the number of entries for each lawmaker (aka how many bills they sponsored)\nWe’re telling it to arrange the data in descending order\n\n\nfinal_table |&gt; \n  group_by(name, district, role, party) |&gt; \n  count(name) |&gt; \n  arrange(desc(n))\n\n\n  \n\n\n\nNow you can see the congressional district associated with each lawmaker. If you want to know what this looks like on a map (because most people don’t know congressional districts off the top of their head), here’s are maps for the Texas House and Texas Senate.\n\n2.1 Just senators\nTo just view Texas senators and their districts, you need to filter the role column. The code is the same as above with the new command added after the group_by line.\n\nsen_districts &lt;- final_table |&gt; \n  group_by(name, district, role, party) |&gt; \n  filter(role == \"Sen\") |&gt; \n  count(name) |&gt; \n  arrange(desc(n))\n\n\nsen_districts\n\n\n  \n\n\n\n\n\n2.2 Just representatives\nYou know the drill. Filter for “Rep” instead of “Sen” and try it on your own first.\n\n\nCode\nhouse_districts &lt;- final_table |&gt; \n  group_by(name, district, role, party) |&gt; \n  filter(role == \"Rep\") |&gt; \n  count(name) |&gt; \n  arrange(desc(n))\n\n\n\nhouse_districts\n\n\n  \n\n\n\nNice! We’re actually going to make our own senate district map later on, so this is an important step.\n\n\n\n3. Bill status\nLegiScan tracks the status of bills in four ways:\n\nNo progress: a bill that was filed but not actually introduced to the session (These appear as “NA” in the data)\nIntroduced: approved for review in session and pending in one of the chambers\nEngrossed: passed one chamber and sent to the other for review\nPassed: approved by both chambers, headed to the governor’s desk\n\nLet’s see how many bills/resolutions are in each category. This is as of the date the data was last updated by LegiScan, so again my result might look different than yours.\n\nfinal_table |&gt; \n  group_by(status_desc) |&gt; \n  count(status_desc) |&gt; \n  arrange(desc(n))\n\n\n  \n\n\n\nAs you can see, most bills are at the status Introduced, meaning they’re still being reviewed in the first chamber and haven’t gone to an official vote. Or maybe there was a vote without enough support and the bill died on the floor. If you’re running this analysis much later in the session, or after the fact, you’ll probably see a higher number for engrossed and passed.\nYou might be thinking, wow a lot of bills made it to the governor. Those are almost exclusively resolutions that memorialize people, commemorate anniversaries or recognize certain days in honor of something. They usually only have to be approved by one chamber, which is why they move through the legislative process so quickly.\n\n\n4. Keyword detection\nMaybe you have a specific bill or keyword in mind that you want to quickly search for. You technically don’t need code to do this. You should be able to see the final_table object listed in the environment window (top right). Click on it to open it in the source window. From there, you can use the search bar to look for specific bill numbers, lawmaker names, keywords in the description, etc.\nIf you want to create a new table of bills that include certain keywords, though, you’ll do so in a code chunk using a more extensive filtering method called filter(grepl()). Without getting too technical, you’re going to tell R what word to look for and in which column.\nLet’s see how many bills have the word “immigration” in the description. Create a code block and run the following:\n\nfinal_table |&gt; \n  filter(grepl(\"immigration\", description, ignore.case = TRUE))\n\n\n  \n\n\n\nNow you can see every bill with a description that includes the word “immigration.” If you want to search for more than one word at a time, add “|” in between each word. For example:\n\nfinal_table |&gt; \n  filter(grepl(\"immigration|border|visa|refugee\", description, ignore.case = TRUE))\n\n\n  \n\n\n\n\n4.1 On your own\nExperiment with this function by filtering for other keywords. To change the column name you’re filtering, replace “description” with the name of the column.\n\n\n\n\n\n\nNote\n\n\n\nThe LegiScan data comes from the bill description only, not the full bill text. This is important to keep in mind as you search for key words.\n\n\n\n\n\n5. Committees\nThere are 54 different committees between the House and the Senate in the 89th session, each responsible for reviewing different types of policy. To see which committee was assigned the most bills, run this code in a new block:\n\nbills_per_committee &lt;- final_table |&gt; \n  group_by(committee) |&gt; \n  summarise(bills = n()) |&gt; \n  arrange(desc(bills))\n\n\nbills_per_committee\n\n\n  \n\n\n\nLooks like about half of the bills haven’t been assigned to committee, which makes sense given how many there are in session. In my results, the House State Affairs Committee has the most, but not by much. Other committees that have been assigned a lot of bills include House Public Education and Senate State Affairs. This would be an interesting thing to re-run later in the session, or after it ends, to see how the data changed.\n\n5.1 Number of Senate committees\nFilter for every committee name that includes “Senate.”\n\nbills_per_committee |&gt; \n  filter(grepl(\"Senate\", committee, ignore.case = TRUE)) |&gt; \n  count()\n\n\n  \n\n\n\n\n\n5.2 Number of House committees\nNow repeat that in a new block, but filter for House.\n\n\nCode\nbills_per_committee |&gt; \n  filter(grepl(\"House\", committee, ignore.case = TRUE)) |&gt; \n  count()\n\n\n\n  \n\n\n\nThis result makes sense, given there are 150 representatives and only 31 senators.\n\n\n\nSaving data for charts\nYou’re going to use some of the data from this section to build our two charts, which means you need to save those tables to your computer. The write_csv() function takes a specific object and saves it as a csv file to your computer.\nDo this with the most_spons object, which is the table of top 10 sponsors that you saved earlier. The format for write_csv is ([object name], “[path to where csv should save]”). Copy and run the following code to save most_spons to your data-processed folder.\n\nwrite_csv(most_spons, \"data-processed/most-spons.csv\")\n\nNow do the same for the sen_districts object.\n\nwrite_csv(most_spons, \"data-processed/sen-districts.csv\")\n\nClick inside your data-processed folder to make sure those appear. AND SAVE YOUR ANALYSIS FILE AGAIN.\n\nExport CSVs from posit.cloud\n\nOpen the data-processed folder.\nCheck the boxes for the two csv files you just created.\nClick the blue settings button, and hit Export.\nName the zip file something that’s easy to find later, like “processed-lege-tables” and hit Download.\n\nMake sure to do this before moving on to the next section.\n\n\n\nRecap\nPat yourself on the back because you just did so much great work! On top of answering all of the questions from the brainstorm, you learned a bunch of functions that are helpful for any data analysis:\n\ngroup_by()\nsummarise()/summarize() … both appear in R and they do the same thing\narrange() and arrange(desc())\ncount()\nfilter() and filter(grepl()\n\nYou also learned how to save objects and run them again, which is a good way to store the changes you made to the data. Lastly, you learned how to turn objects into csv files using write_csv.\nAnd hopefully you learned something new about the status of the Texas session! In the next section, you’ll create two beautiful charts with no coding needed.",
    "crumbs": [
      "Analyzing the Data"
    ]
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Cleaning The Data",
    "section": "",
    "text": "It’s very rare that a dataset comes in the exact format that you want it. There might be unnecessary/distracting columns (as you’ll soon see with the Texas Lege data), empty cells, confusing column names, etc. This is why it’s always good practice to “clean” the data and get it in proper shape before you start analyzing it. How do you clean it? With code! You can use simple lines of code to rename columns, remove or select columns, merge data from separate tables together, and more.\nBefore we get into that, let’s open the README file to learn about the variables included in each dataset.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#setup",
    "href": "cleaning.html#setup",
    "title": "Cleaning The Data",
    "section": "Setup",
    "text": "Setup\nBefore we start coding, we need to install and load the packages that are going to help facilitate the data analysis. You don’t need to worry about what each package does, but this is an important setup step that allows us to use specific functions later on. I always include tidyverse and janitor at the top of every R file.\nIf you want to learn more about what each of those packages does, you can read about them here and here. But again, not super necessary at the moment.\n\nInstalling the packages\nExpand your console at the bottom of the page, type the following command (in the console, not the terminal), and hit enter. Make sure the name of the package is in quotes, or you’ll get an error!\ninstall.packages(\"tidyverse\")\nIt’s going to immediately start spewing out a bunch of scary red text – don’t worry, that actually means it’s working. It’s installing all of the tidyverse packages (and there are quite a few) to the project so you can run them later on. It may take a minute or two to finish. You’ll know it’s done if you see the message “The downloaded source packages are in” and this symbol reappears: &gt;\nNext, do the same thing for janitor.\ninstall.packages(\"janitor\")\nThis one shouldn’t take as long.\n\n\nLoad them into the cleaning file\nGreat! Click your cleaning.qmd file to open it up. At the top, write a little description to remind your future self what this file was for, something like “In this file I cleaned the bills, people, and sponsors data from LegiScan and merged the three together.”\nNext, insert a code chunk (see tip below). Throughout this lesson, I recommend writing a sentence above every code chunk that explains what the code does; it’s a good way of tracking your work for your future self. For example, above the chunk you just added write “I’m loading the tidyverse and janitor packages to this file.”\n\n\n\n\n\n\nTip\n\n\n\nAdd a “code chunk” to your file by hitting Command+Option+i on the keyboard. For PC users, it’s ctrl + alt + i. You’ll run all of your code inside these little blocks, so get used to this shortcut!\n\n\nCopy and paste (or type) the code below and “run” it by hitting the green arrow in the top right of your code chunk. As you can see below, a message pops up when you run it. You don’t need to pay much attention to that; as long as it doesn’t say “Error” you’re fine to move on.\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\n\n\n\nTip\n\n\n\nTo run code without clicking on the green arrow, hit Command + Enter on the keyboard. Another good one to get used to.\n\n\nAnd that’s all you need to worry about for setup!\n\n\n\n\n\n\nImportant\n\n\n\nposit.cloud does not auto save!! So make sure to hit Command + S/ctrl + S frequently to save your work as you go.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#importing-the-data-from-legiscan",
    "href": "cleaning.html#importing-the-data-from-legiscan",
    "title": "Cleaning The Data",
    "section": "2. Importing the data from LegiScan",
    "text": "2. Importing the data from LegiScan\nNow you need to take the data that we downloaded to your computer and import it into this R file. It sounds complicated, but all it takes is a simple line of code.\n\n2.1 Create your data folders\nIf you open the files we downloaded from LegiScan in the previous step, you’ll see it actually includes seven different data tables. For the purposes of this project, we’re only going to be working with three of those: “bills,” “sponsors,” and “people.”\nCreate a new code chunk using the keyboard shortcut from above. Copy and run the code below to import those three tables into this file.\n\n bills &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/bills.csv\")\n\nRows: 9897 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): bill_number, status_desc, title, description, committee, last_acti...\ndbl  (4): bill_id, session_id, status, committee_id\ndate (2): status_date, last_action_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n bills\n\n\n  \n\n\n\n\nsponsors &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/sponsors.csv\")\n\nRows: 14190 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): bill_id, people_id, position\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsponsors\n\n\n  \n\n\n\n\nlawmakers &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/people.csv\")\n\nRows: 181 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): name, first_name, middle_name, last_name, suffix, nickname, party,...\ndbl  (7): people_id, party_id, role_id, followthemoney_eid, votesmart_id, kn...\nlgl  (1): opensecrets_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlawmakers",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#section",
    "href": "cleaning.html#section",
    "title": "Cleaning The Data",
    "section": "3.",
    "text": "3.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#creating-a-pretty-table-to-work-with",
    "href": "cleaning.html#creating-a-pretty-table-to-work-with",
    "title": "Cleaning The Data",
    "section": "3. Creating a pretty table to work with",
    "text": "3. Creating a pretty table to work with",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#cleaning-those-tables",
    "href": "cleaning.html#cleaning-those-tables",
    "title": "Cleaning The Data",
    "section": "“Cleaning” those tables",
    "text": "“Cleaning” those tables\nOftentimes, data tables aren’t in the best shape when you download them. There might be rows or cells that are missing entries, or there may be columns that aren’t necessary for the analysis you want to do. This is where cleaning comes in: you can use code to adjust the table/remove data you don’t need so the final table is much easier to work with.\n\n1. Bills\nLet’s clean up each of the data tables one at a time. We’ll start with our “bills” data.\n\nA) View the raw data\nBefore we start cleaning up the tables, let’s take another look at what they look like untouched. Run this code to view the table you just imported.\n\nbills \n\n\n  \n\n\n\nadd description of what we’re looking at here\n\n\nB) Remove unnecessary columns and rename columns\n\nbills_clean &lt;- bills |&gt; \n   select(-session_id, -url, -state_link) \n\nbills_clean \n\n\n  \n\n\n\n\n# add this to analysis file\nbills_clean |&gt; \n  group_by(status, status_desc) |&gt; \n  summarise(status_type = n()) |&gt; \n  arrange(desc(status_type))\n\n`summarise()` has grouped output by 'status'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\nDescribe what we’re looking at now.\n\n\n\n2. Lawmakers\n\nA) View the raw data\n\nlawmakers \n\n\n  \n\n\n\nDescription of what we’ve got here.\n\n\nB) Remove unnecessary columns and rename columns\n\nlawmakers_clean &lt;- lawmakers |&gt; \n  select(people_id, name, middle_name, suffix, party, role, district) \n\nlawmakers_clean\n\n\n  \n\n\n\nDescription of what we’re working with now.\n\n\n\n3. Sponsors\n\nA) View the raw data\n\nsponsors\n\n\n  \n\n\n\n\n\nB) Remove unnecessary columns and rename columns\n\nsponsors_clean &lt;- sponsors |&gt; \n  rename(spons_position = position)\n\nsponsors_clean\n\n\n  \n\n\n\nRestate all the changes made.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#saving-the-clean-table",
    "href": "cleaning.html#saving-the-clean-table",
    "title": "Cleaning The Data",
    "section": "5. Saving the clean table",
    "text": "5. Saving the clean table\nAdd instructions for creating a data-processed file.\n\nfinal_table |&gt; \n  write_rds(\"data-processed/bills-89.rds\")\n\nIn the next chapter of this toolkit, we’ll brain storm some potential questions about the session and use the data to find some answers.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#merge-the-tables-together",
    "href": "cleaning.html#merge-the-tables-together",
    "title": "Cleaning The Data",
    "section": "Merge the tables together",
    "text": "Merge the tables together\nNow that we’ve cleaned up each table, we’re going to combine everything together into one big table to use for our analysis. We’ll do this using the merge() function.\nSince we’re combining three different tables, we’ll do this in two steps. First, copy and run the code below (in a new code chunk) to merge the lawmaker and sponsor tables together. I’ve created a new object called “law-spons”\n\nlaw_spons &lt;- merge(lawmakers_clean, sponsors_clean) \n\n\nlaw_spons\n\n\n  \n\n\n\nGreat! That’s most of the data we need, but we’re still missing some information from the bills data. Run the code below in a new chunk to add it.\n\n\n\n\n\n\nNote\n\n\n\nI’ve also renamed the middle_name column to say mid_init because that’s more accurate, and I’m picky.\n\n\n\nfinal_table &lt;- law_spons |&gt; \n  left_join(bills_clean) |&gt; \n  rename(mid_init = middle_name) \n\nJoining with `by = join_by(bill_id)`\n\n\n\nfinal_table\n\n\n  \n\n\n\nAmazing! You should have one big data table with 19 different columns:\n\npeople_id = the unique number associated with each lawmaker\nname = lawmaker’s first and last name\nmid_init = lawmaker’s middle initial\nsuffix = lawmaker’s suffix, if applicable\nparty = political party of each lawmaker\nrole = whether the lawmaker is a senator or a representative\ndistrict = the congressional district associated with each lawmaker\nbill_id = the unique number associated with a specific bill filed this session\nbill_number = standard bill naming in the legislature\nspons_position = for a specific bill, this number indicates where a lawmaker is in the line of sponsors\nstatus_desc = each bill’s progress in the session (as of date of download); No progress, Introduced, Engrossed, or Passed\nstatus = numerical representation of a bill’s status\nstatus_date = date a specific bill achieved the corresponding status\ntitle = shorter description of bill as written in LegiScan\ndescription = longer description of bill as written in LegiScan\ncommittee = which Senate/House committee the bill was referred to\ncommittee_id = unique code associated with each committee\nlast_action = most recent update to bill as of the date you downloaded the data\nlast_action_date = calendar date of that update\n\nPhew! This is a lot of data to work with. We may not end up using everything, but it’s still important to have a full picture of the data available. The table has over 10,000 rows of data, each corresponding with a bill/joint resolution. Now you can see the appeal of sorting through everything in R, rather than manually or in a spreadsheet.\n\n\n\n\n\n\nNote\n\n\n\nThere aren’t actually that many unique bills in session because some of because some of those are “companion bills,” meaning two identical bills, one filed in the House and one filed in the Senate.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "about.html#a-credit-to-the-data-source",
    "href": "about.html#a-credit-to-the-data-source",
    "title": "About This Project",
    "section": "A credit to the data source",
    "text": "A credit to the data source\nInclude some sort of disclaimer that all data for this project came from LegiScan; link to site",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "about.html#about-the-author",
    "href": "about.html#about-the-author",
    "title": "About This Project",
    "section": "About the author",
    "text": "About the author\nThis is an independent project by Sarah Brager, a senior journalism student at The University of Texas at Austin. She created this project as her senior capstone for the Moody College Honors Program and UT’s Bridging Disciplines Program, where she’s also earning a certificate in public policy studies. Sarah is a data journalism fellow with UT’s Media Innovation Group. To learn more about Sarah and see some of her other work, view her LinkedIn at the top right of this website.",
    "crumbs": [
      "About This Project"
    ]
  },
  {
    "objectID": "author.html",
    "href": "author.html",
    "title": "Sarah Brager",
    "section": "",
    "text": "Sarah Brager is a graduating senior at The University of Texas at Austin. She created this project as her senior honors capstone. She is currently a data journalism fellow with the UT Media Innovation Group, which is part of the Dallas Morning News Journalism Innovation Endowment.\nWith an interest in political and environmental reporting, Sarah plans to keep using data as an investigative tool in her career. To learn more about Sarah and view her other work, visit her LinkedIn and GitHub profiles.",
    "crumbs": [
      "Meet the Author"
    ]
  },
  {
    "objectID": "index.html#about-this-project",
    "href": "index.html#about-this-project",
    "title": "Reporting on the Texas Legislature Using Data: A Digital Toolkit for Journalists",
    "section": "About this project",
    "text": "About this project\n\nWho is this toolkit for?\nAnyone who wants to learn to work with numbers! While the tutorial is geared towards journalists (both students and professionals), anyone who wants to practice using R can benefit from the lesson. Maybe you’re not a data journalist/not interested in a data career, but these are still great digital skills for anyone who does research or works with large spreadsheets. Also a plus if you’re a government/politics nerd because that’s the focus of our data today.\n\n\n\n\n\n\nNote\n\n\n\nIf you already have intermediate experience using R for data analysis, this tutorial will likely be too easy for you. But feel free to jump around in the lesson or run your own computations with the dataset!\n\n\n\n\nWho created this lesson?\nThis toolkit was created by Sarah Brager, a graduating journalism and public policy student at the University of Texas at Austin. She designed this project for her senior honors capstone. To learn more about Sarah, visit the “Meet the Author” page on the left sidebar.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#toolkit-objectives",
    "href": "index.html#toolkit-objectives",
    "title": "Reporting on the Texas Legislature Using Data: A Digital Toolkit for Journalists",
    "section": "Toolkit objectives",
    "text": "Toolkit objectives\n\nBy the end of this tutorial, you will have:\n\nLearned more about data journalism practices\nFamiliarized yourself with working in R\nGained tips for finding and downloading public data\nUsed code to analyze the data and note important trends\nConvey some of those trends in two types of charts\nBrainstormed potential ways to use this data in news production",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "author.html#education",
    "href": "author.html#education",
    "title": "Sarah Brager",
    "section": "Education",
    "text": "Education\nThe University of Texas at Austin | Aug 2021 - May 2025\nBachelor of Journalism | Certificate in Public Policy Studies\nMoody College Honors Program | Bridging Disciplines Program",
    "crumbs": [
      "Meet the Author"
    ]
  },
  {
    "objectID": "author.html#about",
    "href": "author.html#about",
    "title": "Sarah Brager",
    "section": "",
    "text": "Sarah Brager is a graduating senior at The University of Texas at Austin. She created this project as her senior honors capstone. She is currently a data journalism fellow with the UT Media Innovation Group, which is part of the Dallas Morning News Journalism Innovation Endowment.\nWith an interest in political and environmental reporting, Sarah plans to keep using data as an investigative tool in her career. To learn more about Sarah and view her other work, visit her LinkedIn and GitHub profiles.",
    "crumbs": [
      "Meet the Author"
    ]
  },
  {
    "objectID": "cleaning.html#read-about-each-csv",
    "href": "cleaning.html#read-about-each-csv",
    "title": "Cleaning The Data",
    "section": "Read about each CSV",
    "text": "Read about each CSV\nClick on the README.md file on the right side to open the document. It should appear in the top left window. If you want to collapse the console/terminal for a better view, click on the small box in the top right corner of the console window.\nNow you can scroll through the README and learn more about each dataset, including the column names and a description of what the data in that column is. The README comes directly from the developers at LegiScan; this is a nice explanatory document that many databases will provide with raw data, but it isn’t always included.\n\nAs I said in the last section, we’re only going to be working with the “bills,” ”people,” and ”sponsors” datasets, so definitely take a look at those. If you want to close the document, click on the small”x” next to the file name (right above the toolbar). But you can also keep it open as we work in our cleaning file.",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#import-the-data-into-this-file",
    "href": "cleaning.html#import-the-data-into-this-file",
    "title": "Cleaning The Data",
    "section": "Import the data into this file",
    "text": "Import the data into this file\nNow you need to take the data that we downloaded to your computer and import it into this R file. It sounds complicated, but all it takes is a simple line of code.\n\n2.1 Create your data folders\nIf you open the files we downloaded from LegiScan in the previous step, you’ll see it actually includes seven different data tables. For the purposes of this project, we’re only going to be working with three of those: “bills,” “sponsors,” and “people.”\nCreate a new code chunk using the keyboard shortcut from above. Copy and run the code below to import those three tables into this file.\n\n bills &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/bills.csv\")\n\nRows: 9897 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): bill_number, status_desc, title, description, committee, last_acti...\ndbl  (4): bill_id, session_id, status, committee_id\ndate (2): status_date, last_action_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n bills\n\n\n  \n\n\n\n\nsponsors &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/sponsors.csv\")\n\nRows: 14190 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): bill_id, people_id, position\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsponsors\n\n\n  \n\n\n\n\nlawmakers &lt;- read_csv(\"data-raw/tx-bills-all/2025-2026_89th_Legislature/csv/people.csv\")\n\nRows: 181 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): name, first_name, middle_name, last_name, suffix, nickname, party,...\ndbl  (7): people_id, party_id, role_id, followthemoney_eid, votesmart_id, kn...\nlgl  (1): opensecrets_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlawmakers",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#save-the-clean-table",
    "href": "cleaning.html#save-the-clean-table",
    "title": "Cleaning The Data",
    "section": "Save the clean table",
    "text": "Save the clean table\nBefore we move on to the analysis, you need to save the final table to your data-processed folder. To do this, you’re going to tell R to create an Rds file, which saves final_table object you created earlier. Use the write_rds function, and inside the parenthesis, include the location (data-processed folder) and a name for your new rds file. I chose to name mine “bills-89” but feel free to name yours something else as long as it makes sense. The code looks like this:\n\nfinal_table |&gt; \n  write_rds(\"data-processed/bills-89.rds\")\n\nNothing appears when you run it, but if you click into your data-processed folder, you should see the Rds file.\n\n\n\n\n\n\nImportant\n\n\n\nHave you saved your changes to this file? Hit Command/ctrl + S!",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#load-the-raw-data",
    "href": "cleaning.html#load-the-raw-data",
    "title": "Cleaning The Data",
    "section": "Load the raw data",
    "text": "Load the raw data\nNext, you need to tell R that you want it to read the csv files from your data-raw folder so they appear below. The function to do this is aptly named read_csv().\nCreate a new code chunk and add a sentence above it explaining that you’re importing the three csv files.\nRun the code below to import the “bills” data into this file. No data should appear yet, but you’ll see a message from R pop up. By running this code, you’ve created an object called “bills” and have told R that you want to read the csv and save that data inside the new object.\n\nbills &lt;- read_csv(\"data-raw/bills.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nBy putting “data-raw/bills.csv” inside the function, you’re basically telling R: inside the data-raw folder, read the file bills.csv\n\n\nIn the same code chunk, hit Enter twice so you’re on a new line. Type bills and run the code again. You should see a table with 14 columns and thousands of rows of data! Click through it.\n\nbills \n\n\n  \n\n\n\nRepeat these two steps for the “sponsors” data and “people” data.\n\nUse read_csv to import the data and save it in an object. Run that code.\nOn a new line, type the name of the object and run it so the table appears.\n\nRemember to leave descriptions of your work and save as you go!\n\nsponsors &lt;- read_csv(\"data-raw/sponsors.csv\")\n\n\nsponsors \n\n\n  \n\n\n\n\nlawmakers &lt;- read_csv(\"data-raw/people.csv\")\n\n\nlawmakers",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#clean-the-tables",
    "href": "cleaning.html#clean-the-tables",
    "title": "Cleaning The Data",
    "section": "Clean the tables",
    "text": "Clean the tables\nYou probably noticed while clicking through that these tables aren’t as clean as they could be. Some of the column names don’t make sense, there are a lot of “NA” cells, and some of the information is just unnecessary for what we want to look at in this project. Let’s go through each table one at a time and clean them up.\n\n1. Bills\nIf you go back and look at the bills table, you can see it includes a lot of different columns, some of which are just links. We’re going to use the select() function to remove those columns. We can also take out the session_id column because we know all of this data is from the 89th legislative session, so that value is going to be the same for every entry.\nCreate a new code block, name it, and then copy and run the code below. By putting a minus sign in front of the column name, you’re telling R: don’t include this column in the new table.\n\nbills_clean &lt;- bills |&gt; \n   select(-session_id, -url, -state_link) \n\n\nbills_clean\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how I saved this in a new object called “bills_clean.” This means the untouched data still exists in the object “bills” if you ever wanted to run that and view it again. It’s good practice to create new objects as you change the data and rename it to something that makes sense.\n\n\nNow you can click through the table to see that the url, state link, and session id data is gone.\n\n\n2. Lawmakers (people)\nLooking at the people data, which we’ve saved as “lawmakers,” you can see there are a lot of specific ids for other open data portals, like Follow the Money, Open Secrets, Ballotpedia, and KnowWho. While these are cool to have, we don’t need them for the analysis we’re running today. If you were writing a story about legislative campaign funds in Texas, for example, you might chose to keep the Follow the Money column.\nTo keep things simple (and to make our table a little smaller) let’s remove those columns. We’re still going to use the select() function by in the opposite way, this time telling R which columns you do want to include.\n\nlawmakers_clean &lt;- lawmakers |&gt; \n  select(people_id, name, middle_name, suffix, party, role, district) \n\n\nlawmakers_clean\n\n\n  \n\n\n\nYou could do this the same way we did for bills, which was use select(-) and tell R the names of the columns you want to remove. It doesn’t really matter, I just wanted to demonstrate both options.\n\n\n3. Sponsors\nSponsors is a much smaller data table, with only three columns. As noted in the README, “position” refers to where a lawmaker appears on the sponsor list for a specific bill. For example, if a lawmaker is the primary sponsor for a bill, their position number would read “1” … if they were the second person to sponsor that bill, it would read “2” … etc.\nAll three of these columns will be useful to us, so we’re not going to remove anything.\nHowever, this is a good chance to try the rename() function, which allows you the change the name of a column. Personally, I want to remind myself that the position number is associated with sponsor order, so I want to rename the column to “spons_position”\n\nsponsors_clean &lt;- sponsors |&gt; \n  rename(spons_position = position)\n\n\nsponsors_clean\n\n\n  \n\n\n\nNow you can see the new column name when you run the table.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to choose a name that makes more sense to you, use this convention in the code: rename([new column name] = [original column name])",
    "crumbs": [
      "Cleaning the Data"
    ]
  },
  {
    "objectID": "cleaning.html#recap",
    "href": "cleaning.html#recap",
    "title": "Cleaning The Data",
    "section": "Recap",
    "text": "Recap\nThis section had a lot of information! Here are the key takeaways:\n\nREADME files are helpful resources to learn what’s included in a dataset.\nSetup is an important first step for running code. You learned how to install packages in your console and load those into your R file using the library() function.\nYou need to tell R which file you want it to read and spit out. Use the read_csv function and include a direct path to the location of your file.\nClean up data using a couple helpful functions:\n\nselect() lets you indicate which columns you do or don’t want to include\nrename() lets you change the column names to whatever you prefer\n\nCombine columns from different tables using merge()\nSave objects in an Rds file using write_rds, and make sure to direct these to your data-processed folder.\n\nIn the next section, we’ll brain storm some questions about the Texas Legislative Session and try to answer them with some simple data computations.",
    "crumbs": [
      "Cleaning the Data"
    ]
  }
]